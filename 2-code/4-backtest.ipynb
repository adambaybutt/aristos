{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6359323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO DELETE PACKAGES I DONT END UP USING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207651c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitOLS(train_df, lhs_col, rhs_cols):\n",
    "    # Obtain the training input and output data and, if passed, validation data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "\n",
    "    # add constant\n",
    "    train_rhs = np.concatenate((np.ones([len(train_lhs),1]), train_rhs),\n",
    "                               axis=1)\n",
    "\n",
    "    # fit\n",
    "    betahat = np.matmul(np.linalg.inv(np.matmul(train_rhs.T, train_rhs)),\n",
    "                        np.matmul(train_rhs.T, train_lhs))\n",
    "\n",
    "    return betahat\n",
    "\n",
    "\n",
    "def genOLSYhats(oos_df, model, lhs_col, rhs_cols):    \n",
    "    # Obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    oos_lhs   = oos_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # add constant\n",
    "    oos_rhs   = np.concatenate((np.ones([len(oos_lhs),1]), oos_rhs),\n",
    "                                axis=1).reshape(-1)\n",
    "    \n",
    "    # Form the yhats\n",
    "    yhats = np.dot(model, oos_rhs)\n",
    "    \n",
    "    # Return results\n",
    "    return pd.DataFrame(data={'date': oos_df.index.values,\n",
    "                              'y': np.array(oos_df['y'].values),\n",
    "                              'yhats': yhats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e614cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLasso(train_df, hps_yhats_dict, lhs_col, rhs_cols):\n",
    "    # Extract hps\n",
    "    alpha = hps_yhats_dict['alpha']\n",
    "\n",
    "    # Obtain the training input and output data and, if passed, validation data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "\n",
    "    # standardize\n",
    "    scaler = StandardScaler()\n",
    "    train_rhs = scaler.fit_transform(train_rhs)\n",
    "\n",
    "    # fit\n",
    "    model = linear_model.Lasso(alpha=alpha, \n",
    "                               selection='cyclic')\n",
    "    model.fit(train_rhs, train_lhs)\n",
    "\n",
    "    # gather what coefs were used\n",
    "    used_coefs = (model.coef_!=0)*1\n",
    "\n",
    "    return model, used_coefs, scaler\n",
    "\n",
    "def genLassoYhats(oos_df, model, lhs_col, rhs_cols, scaler):    \n",
    "    # Obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    oos_lhs   = oos_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # Standarize\n",
    "    oos_rhs   = scaler.transform(oos_rhs)\n",
    "    \n",
    "    # Form the yhats\n",
    "    yhats = model.predict(oos_rhs)\n",
    "    \n",
    "    # Return results\n",
    "    return pd.DataFrame(data={'date': oos_df.index.values,\n",
    "                              'y': np.array(oos_df['y'].values),\n",
    "                              'yhats': yhats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "69edbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitGBDT(train_df, hps_yhats_dict, lhs_col, rhs_cols):\n",
    "    # Extract the hyperparameters\n",
    "    learning_rate  = hps_yhats_dict['learning_rate']\n",
    "    num_est        = hps_yhats_dict['num_est']\n",
    "    subsample      = hps_yhats_dict['subsample']\n",
    "    min_samp_split = hps_yhats_dict['min_samp_split']\n",
    "    max_depth      = hps_yhats_dict['max_depth']\n",
    "    max_feature    = hps_yhats_dict['max_feature']\n",
    "    random_state   = int(learning_rate*num_est*subsample*min_samp_split*max_depth*max_feature)\n",
    "\n",
    "    # Obtain the training input and output data and, if passed, validation data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # Build the model\n",
    "    model = GradientBoostingClassifier(loss='log_loss',\n",
    "                                       learning_rate=learning_rate,\n",
    "                                       n_estimators=num_est,\n",
    "                                       subsample=subsample,\n",
    "                                       min_samples_split=min_samp_split,\n",
    "                                       max_depth=max_depth,\n",
    "                                       max_features=max_feature,\n",
    "                                       verbose=0,\n",
    "                                       random_state=random_state)\n",
    "\n",
    "    # Fit\n",
    "    model.fit(X=train_rhs, y=train_lhs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genYhats(oos_df, model, lhs_col, rhs_cols):    \n",
    "    # Obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    oos_lhs   = oos_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # Form the yhats\n",
    "    yhats = model.predict_proba(oos_rhs)\n",
    "    yhats = 2*(yhats[:,1]-0.5) # map from 0 to 1 prob to -1 to 1 window and can adjust cutoff after\n",
    "    \n",
    "    # Return results\n",
    "    return pd.DataFrame(data={'date': oos_df.index.values,\n",
    "                              'y': np.array(oos_df['y'].values),\n",
    "                              'yhats': yhats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df, last_train_year=2018, val_end_year=2021, arch_name=None, num_cpus=10):\n",
    "    #Initialize hp result objects\n",
    "    results_list    = []\n",
    "    hps_mse_df_list = []\n",
    "\n",
    "    # determine columns\n",
    "    all_cols = list(df.columns.values)\n",
    "    lhs_col  = ['y']\n",
    "    all_cols.remove('y')\n",
    "    rhs_cols = all_cols\n",
    "    assert(df.shape[1]==(len(lhs_col)+len(rhs_cols))) # lhs col + rhs cols\n",
    "\n",
    "    # Initialize the hyperparameter grid for GBDT\n",
    "    learning_rates  = [0.1, 0.05, 0.01] # 0.1, 0.05, 0.01 # [5e-2, 5e-3, 5e-4, 5e-5]\n",
    "    num_ests        = [250] # [100, 250, 500, 1000] \n",
    "    subsamples      = [0.99]  # [0.95, 0.99, 0.999]\n",
    "    min_samp_splits = [0.005] # [0.001, 0.005, 0.01]\n",
    "    max_depths      = [2, 3, 4] # [2, 3, 4, 5]\n",
    "    max_features    = [0.1, 0.15, 0.2] # [0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25]\n",
    "\n",
    "    # Initialize the hyperparameter grid for Lasso\n",
    "    alphas          = [5e-3] # [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n",
    "\n",
    "    # Initialize hyperparameter grid for OLS\n",
    "    window_sizes    = [1000, 5000, 10000, 20000, 35000, 50000, 65000]\n",
    "\n",
    "    # Determine the dates in the validation window  \n",
    "    val_dates = np.unique(df[(df.index.year > last_train_year)  \n",
    "                             & (df.index.year <= val_end_year)].index.values) \n",
    "\n",
    "\n",
    "    # Generate yhats for every hyperparameter grid point\n",
    "    #     for hps in itertools.product(learning_rates,\n",
    "    #                                  num_ests,\n",
    "    #                                  subsamples,\n",
    "    #                                  min_samp_splits,\n",
    "    #                                  max_depths, \n",
    "    #                                  max_features):\n",
    "    #         # build the hp and yhats dictionary\n",
    "    #         hps_yhats_dict = {'learning_rate': hps[0],\n",
    "    #                           'num_est': hps[1],\n",
    "    #                           'subsample': hps[2],\n",
    "    #                           'min_samp_split': hps[3],\n",
    "    #                           'max_depth': hps[4],\n",
    "    #                           'max_feature': hps[5],\n",
    "    #                           'results_df': pd.DataFrame()}\n",
    "    for hps in itertools.product(window_sizes):\n",
    "        hps_yhats_dict = {'window_size': hps[0],\n",
    "                          'results_df': pd.DataFrame()}\n",
    "\n",
    "        # fit on all val dates\n",
    "        print(hps_yhats_dict, '\\n')\n",
    "        tic = time.perf_counter()\n",
    "        def loopOverValDates(val_date):\n",
    "            # Form train and validation data frames\n",
    "            window_size = hps_yhats_dict['window_size']\n",
    "            temp_df  = df[lhs_col+rhs_cols][df.index <= val_date]\n",
    "            train_df = temp_df[temp_df.index < val_date][-window_size:]\n",
    "            val_df   = temp_df[temp_df.index == val_date]\n",
    "\n",
    "            # fit model and generate yhats for val week\n",
    "            model          = fitOLS(train_df, lhs_col, rhs_cols)\n",
    "            val_results_df = genOLSYhats(val_df, model, lhs_col, rhs_cols)\n",
    "\n",
    "            # Return this date's results\n",
    "            return val_results_df, model\n",
    "\n",
    "        # Fit on all the val dates\n",
    "        val_results = Parallel(n_jobs=num_cpus)(delayed(loopOverValDates)(val_date) for val_date in tqdm(val_dates))\n",
    "\n",
    "        # Extract val results\n",
    "        val_results_df = pd.DataFrame()\n",
    "        beta_hats       = np.zeros((len(rhs_cols)+1))\n",
    "        for j in range(len(val_results)):\n",
    "            val_results_df = pd.concat((val_results_df, val_results[j][0]))\n",
    "            beta_hats      += val_results[j][1]\n",
    "        hps_yhats_dict['results_df'] = val_results_df\n",
    "        hps_yhats_dict['beta_hats']  = beta_hats/len(val_dates)\n",
    "\n",
    "\n",
    "\n",
    "#         def loopOverValDates(val_date):\n",
    "#             # Form train and validation data frames\n",
    "#             temp_df  = df[lhs_col+rhs_cols][df.index <= val_date]\n",
    "#             train_df = temp_df[temp_df.index < val_date]\n",
    "#             val_df   = temp_df[temp_df.index == val_date]\n",
    "\n",
    "#             # Fit model and generate yhats for val week\n",
    "#             model          = fitGBDT(train_df, hps_yhats_dict, lhs_col, rhs_cols)\n",
    "#             val_results_df = genYhats(val_df, model, lhs_col, rhs_cols)\n",
    "\n",
    "#             # Obtain training data accuracy for studying over/underfit\n",
    "#             train_acc = model.score(train_df[rhs_cols].values, train_df['y'].values)\n",
    "\n",
    "#             # Return this date's results\n",
    "#             return val_results_df, train_acc\n",
    "\n",
    "#         # Fit on all the val dates\n",
    "#         val_results = Parallel(n_jobs=num_cpus)(delayed(loopOverValDates)(val_date) for val_date in tqdm(val_dates))\n",
    "\n",
    "#         # Extract val results\n",
    "#         val_results_df = pd.DataFrame()\n",
    "#         train_accs     = []\n",
    "#         for j in range(len(val_results)):\n",
    "#             val_results_df = pd.concat((val_results_df, val_results[j][0]))\n",
    "#             train_accs.append(val_results[j][1])\n",
    "#         hps_yhats_dict['results_df'] = val_results_df\n",
    "#         hps_yhats_dict['avg_train_acc'] = np.mean(train_accs)\n",
    "\n",
    "        # Save run time and space out result print out\n",
    "        toc = time.perf_counter()\n",
    "        print('\\n\\n\\n')\n",
    "\n",
    "        # Update this HP point's results and append to list of results\n",
    "        # ys    = hps_yhats_dict['results_df'].y.values\n",
    "        # yhats = (hps_yhats_dict['results_df'].yhats.values>0)*1\n",
    "        # yhats[yhats==0] = -1\n",
    "        # oos_acc = np.sum(ys==yhats)/len(yhats)\n",
    "        # hps_yhats_dict['oos_acc'] = oos_acc\n",
    "        results_list.append(hps_yhats_dict)\n",
    "\n",
    "        # Save the ongoing results as a csv to be able to watch\n",
    "        cv_results_dict = hps_yhats_dict.copy()\n",
    "        del cv_results_dict['results_df']\n",
    "        if 'models' in cv_results_dict:\n",
    "            del cv_results_dict['models']\n",
    "        cv_results_dict['runtime_mins']     = round((toc - tic)/60, 0)  \n",
    "        cv_results_dict['arch_name']        = arch_name\n",
    "        cv_results_dict['first_val_year']   = str(last_train_year+1) \n",
    "        cv_results_dict['first_train_year'] = str(np.min(df.index.year))\n",
    "\n",
    "        #temp_df = hps_yhats_dict['results_df'].copy()\n",
    "        # ys      = temp_df[temp_df.date.dt.year==2021][lhs_col].values.reshape(-1)\n",
    "        # yhats   = temp_df[temp_df.date.dt.year==2021].yhats.values\n",
    "        # yhats   = (yhats>0)*1\n",
    "        # yhats[yhats==0] = -1\n",
    "        # cv_results_dict['oos_acc_2021'] = np.sum(ys==yhats)/len(yhats)\n",
    "\n",
    "#         hps_mse_df_list.append(pd.DataFrame(cv_results_dict, index=[0]))\n",
    "#         cv_df = pd.concat(hps_mse_df_list, ignore_index=True)\n",
    "\n",
    "#         timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         fp = '../4-output/cv-results-' + arch_name +'-' + timestr + '.csv'\n",
    "#         cv_df.to_csv(fp, index=False)\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4558459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelPortfolioWeights(df):\n",
    "    # set parameters for tertile weights\n",
    "    # note: can't go to zero on any as there may be week where that is only yhat\n",
    "    bottom_tertile = 1/6\n",
    "    mid_tertile = 1/3\n",
    "    top_tertile = (1-bottom_tertile-mid_tertile)\n",
    "\n",
    "    # create portfolio weights\n",
    "    df = df.sort_values(by=['date', 'yhat'])\n",
    "    df['counts']  = 1\n",
    "    df['total_assets_per_week_tertile'] = df.groupby(['date', 'yhat']).counts.transform('sum')\n",
    "    df.loc[df.yhat == -1, 'prtfl_wght'] = bottom_tertile / df[df.yhat == -1].total_assets_per_week_tertile\n",
    "    df.loc[df.yhat == 0,  'prtfl_wght'] = mid_tertile / df[df.yhat == 0].total_assets_per_week_tertile\n",
    "    df.loc[df.yhat == 1,  'prtfl_wght'] = top_tertile / df[df.yhat == 1].total_assets_per_week_tertile\n",
    "\n",
    "    # clean up\n",
    "    df = df.drop(['counts', 'total_assets_per_week_tertile'], axis=1)\n",
    "\n",
    "    # fix weeks where portfolio weights add up to less than 1\n",
    "    temp_df = df.groupby(['date'])[['prtfl_wght']].sum()\n",
    "    temp_df = temp_df[~np.isclose(temp_df.prtfl_wght, 1)]\n",
    "    if 0<temp_df.shape[0]:\n",
    "        for i in range(temp_df.shape[0]):\n",
    "            date = temp_df.index.values[i]\n",
    "            total_weight = temp_df.prtfl_wght.values[i]\n",
    "            df.loc[df.index == date, 'prtfl_wght'] = df[df.index==date].prtfl_wght * (1/total_weight)\n",
    "            \n",
    "    # confirm portfolio weights roughly sum to 1 for each week\n",
    "    assert(len(np.unique(df.index)) == \n",
    "           np.sum(np.isclose(df.groupby(['date']).prtfl_wght.sum(), 1,\n",
    "                             rtol=1e-2, atol=1e-2)))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAnnualTransactionCosts(df):\n",
    "    # merge on the previous week's holdings to the new holdings\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df[temp_df.index<np.max(temp_df.index)]\n",
    "    temp_df.index = temp_df.index+np.timedelta64(1, 'D')\n",
    "    temp_df = temp_df[['asset', 'prtfl_wght']]\n",
    "    temp_df = temp_df.rename(columns={'prtfl_wght': 'prtfl_wght_tm7'})\n",
    "    df      = df.merge(temp_df,\n",
    "                       on=['date', 'asset'],\n",
    "                       how='outer',\n",
    "                       validate='one_to_one')\n",
    "\n",
    "    # calc weekly turnover and ensure it has the appropriate range\n",
    "    df['asset_to'] = np.abs(df.prtfl_wght - df.prtfl_wght_tm7)\n",
    "    to_df = df.groupby('date')[['asset_to']].sum().reset_index()\n",
    "    assert((np.min(to_df.asset_to)>=0) & (np.max(to_df.asset_to<=2)))\n",
    "\n",
    "    # correct the first and last week valid for buying the initial port and liquidating\n",
    "    to_df.loc[0, 'asset_to'] = 1\n",
    "    to_df = pd.concat((to_df, pd.DataFrame(data={'date': np.max(temp_df.index.values)+np.timedelta64(1, 'D'),\n",
    "                                                 'asset_to': 1}, index=[0])))\n",
    "    to_df = to_df.reset_index(drop=True)\n",
    "\n",
    "    # add transaction costs assuming maker and taker fee of 20 bps each\n",
    "    to_df['tc'] = to_df.asset_to*0.002\n",
    "\n",
    "    # return annualize transaction cost\n",
    "    return -np.sum(to_df.tc)/(to_df.shape[0]/52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioReturn(r_df):\n",
    "    num_days = r_df.shape[0]\n",
    "    if np.sum(r_df.r.values <= -1)>=1:\n",
    "        return -1\n",
    "    else:\n",
    "        tot_ret   = np.product(r_df.r.values+1)-1\n",
    "        daily_ret = (tot_ret+1)**(1/num_days)-1\n",
    "        annl_ret  = (daily_ret+1)**(365.25)-1\n",
    "        return annl_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa790826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioSharpe(r_df):\n",
    "    daily_sharpe = np.mean(r_df.r.values)/np.std(r_df.r.values)\n",
    "    annl_sharpe  = daily_sharpe*np.sqrt(365.25)\n",
    "    return annl_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983905fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_draw_down(r_df):\n",
    "    cumulative_ret=(r_df.r+1).cumprod()\n",
    "    roll_max=cumulative_ret.rolling(len(cumulative_ret), min_periods=1).max()\n",
    "    daily_drawdown=cumulative_ret/roll_max\n",
    "    max_daily_drawdown=daily_drawdown.min() - 1\n",
    "    return max_daily_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a85a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_1_week_loss(r_df):\n",
    "    max_loss=(r_df['r']+1).rolling(7).apply(np.prod)\n",
    "    max_loss_minus=max_loss.min()-1\n",
    "    return max_loss_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTestYhats(df, opt_hps, test_year=2022): \n",
    "    test_dates = np.unique(df[df.index.year == test_year].index.values)\n",
    "\n",
    "    all_cols = list(df.columns.values)\n",
    "    lhs_col  = ['y']\n",
    "    all_cols.remove('y')\n",
    "    rhs_cols = all_cols\n",
    "\n",
    "    def loopOverOOSDates(test_date):\n",
    "        # build data\n",
    "        temp_df       = df[df.index <= test_date].copy()\n",
    "        train_df      = temp_df[temp_df.index < test_date]\n",
    "        oos_df        = temp_df[temp_df.index == test_date]\n",
    "\n",
    "        # fit and predict\n",
    "        model          = fitGBDT(train_df, opt_hps, lhs_col, rhs_cols)\n",
    "        oos_results_df = genYhats(oos_df, model, lhs_col, rhs_cols)\n",
    "\n",
    "        return oos_results_df\n",
    "\n",
    "    # run\n",
    "    oos_results = Parallel(n_jobs=num_cpus)(delayed(loopOverOOSDates)(test_date) for test_date in tqdm(test_dates))\n",
    "\n",
    "    # Extract oos results\n",
    "    oos_results_df = pd.concat(oos_results)\n",
    "\n",
    "    return oos_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE CODE\n",
    "\n",
    "# Load in the data\n",
    "input_fp = '../3-data/clean/panel_btceth_30min.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "\n",
    "# Clean up the data \n",
    "df = df.set_index('date')\n",
    "df = df.astype('float32')\n",
    "\n",
    "# Select useful variables accoridng to Lasso\n",
    "df = df[['y',\n",
    "         'covar_btc_volume_sum_tm288',\n",
    "         'covar_btc_r_tm36',\n",
    "         'covar_btc_ema_144_t',\n",
    "         'covar_btc_ema_288_t',\n",
    "         'covar_eth_r_tm12',\n",
    "         'covar_eth_ma_24_t',\n",
    "         'covar_btc_rsi_tm288',\n",
    "         'covar_eth_rsi_tm2016',\n",
    "         'covar_eth_rsi_tm4032',\n",
    "         'macro_mcap_ret_ath_t',\n",
    "         'macro_mcap_altcoin_ret_ath_t']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV\n",
    "results_list = runCV(df, last_train_year=2019, val_end_year=2021,\n",
    "                     arch_name='lasso_longshort', num_cpus=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # step 1\n",
    "\n",
    "    # step 2\n",
    "\n",
    "    # step 3\n",
    "\n",
    "    # do gbdt with the full set where i randomly sample sqrt of them and then try with the 36 cols from lasso where i sample half or something\n",
    "    \n",
    "# played with the lasso code below to select a final set of 36 features\n",
    "    lasso_feats =  ['covar_btc_covar_trades_t_ma_tm8640',\n",
    "                    'covar_btc_covar_trades_t_min_tm30',\n",
    "                    'covar_btc_covar_trades_t_min_tm60',\n",
    "                    'covar_btc_covar_volume_t_min_tm4320',\n",
    "                    'covar_btc_p_log_t',\n",
    "                    'covar_btc_r_1min_ema_tm360',\n",
    "                    'covar_btc_r_5min_skew_tm720',\n",
    "                    'covar_btc_r_tm1440',\n",
    "                    'covar_btc_r_tm15',\n",
    "                    'covar_btc_r_tm86400',\n",
    "                    'covar_btc_rsi_tm720',\n",
    "                    'covar_eth_covar_trades_t_max_tm4320',\n",
    "                    'covar_eth_covar_trades_t_min_tm20',\n",
    "                    'covar_eth_covar_trades_t_vol_tm60',\n",
    "                    'covar_eth_covar_volume_t_ma_tm60',\n",
    "                    'covar_eth_covar_volume_t_ma_tm8640',\n",
    "                    'covar_eth_covar_volume_t_max_tm4320',\n",
    "                    'covar_eth_covar_volume_t_max_tm720',\n",
    "                    'covar_eth_covar_volume_t_sum_tm20',\n",
    "                    'covar_eth_covar_volume_t_sum_tm60',\n",
    "                    'covar_eth_covar_volume_t_vol_tm360',\n",
    "                    'covar_eth_p_t',\n",
    "                    'covar_eth_r_1min_ema_tm60',\n",
    "                    'covar_eth_r_1min_ma_tm360',\n",
    "                    'covar_eth_r_1min_vol_tm20',\n",
    "                    'covar_eth_r_1min_vol_tm5',\n",
    "                    'covar_eth_r_5min_ma_tm20160',\n",
    "                    'covar_eth_r_5min_ma_tm60',\n",
    "                    'covar_eth_r_5min_min_tm30',\n",
    "                    'covar_eth_r_5min_min_tm720',\n",
    "                    'covar_eth_r_cummax_t',\n",
    "                    'covar_eth_r_cummin_t',\n",
    "                    'covar_eth_r_tm120',\n",
    "                    'covar_eth_r_tm1440',\n",
    "                    'covar_eth_r_tm40320',\n",
    "                    'covar_eth_volume_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_master = results_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ca09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE RESULTS\n",
    "# with open('../3-data/derived/validation_2021_results_btceth_ols.pickle', 'wb') as handle:\n",
    "#     pickle.dump(results_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa665ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study for signal\n",
    "for i in [6, 5, 4, 3, 2, 1, 0]:\n",
    "    print(i)\n",
    "    df             = pd.read_pickle(input_fp)\n",
    "    val_results_df = results_list[i]['results_df']\n",
    "    val_results_df = val_results_df.drop('y', axis=1)\n",
    "    val_results_df = val_results_df.merge(df,\n",
    "                                          on=['date'],\n",
    "                                          how='inner', \n",
    "                                          validate='one_to_one')\n",
    "    val_df = pd.DataFrame(data={'date': val_results_df.date.values,\n",
    "                                'y': val_results_df.y.values,\n",
    "                                'y_btc_eth_r_tp2_tp7': val_results_df['y_btc_eth_r_tp2_tp7'].values,\n",
    "                                'yhat': val_results_df.yhats.values})\n",
    "\n",
    "    val_df['yhat'] = (val_df.yhat-np.min(val_df.yhat))\n",
    "    val_df['yhat'] = val_df.yhat/np.max(val_df.yhat)\n",
    "    for lev in [2, 3, 4, 5]:\n",
    "        for fee in [1e-4, 2e-4, 5e-4]:\n",
    "            print(lev)\n",
    "            print(fee)\n",
    "            yhat_vals = val_df.yhat.values\n",
    "            for yhat_val in yhat_vals:\n",
    "                val_df.loc[val_df.yhat<=yhat_val, 'pw'] = -1\n",
    "                val_df.loc[val_df.yhat>yhat_val, 'pw'] = 1\n",
    "                assert(0==val_df.pw.isnull().sum())\n",
    "                port_ret = np.prod(lev*(1+val_df.pw*val_df.y_btc_eth_r_tp2_tp7)-fee)**(1/len(val_df))-1\n",
    "                val_df.loc[val_df.yhat==yhat_val, 'ret'] = port_ret\n",
    "\n",
    "            val_df = val_df.drop('pw', axis=1)\n",
    "\n",
    "            val_df = val_df.sort_values(by='yhat')\n",
    "            val_df['yhat'] = np.arange(len(val_df))/len(val_df)\n",
    "            plt.plot(val_df.yhat, val_df.ret)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345dcfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10\n",
    "val_df['yhat_decile'] = pd.qcut(val_df.yhat, q=bins, labels=np.arange(bins))\n",
    "val_df.groupby('yhat_decile')['y_btc_eth_r_tp2_tp7'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study feature importance\n",
    "\n",
    "feat_imprt = np.zeros(df.shape[1]-2)\n",
    "models = results_list[0]['models']\n",
    "for model in models:\n",
    "    feat_imprt += model.feature_importances_\n",
    "feat_imprt = feat_imprt/(len(models)-1)\n",
    "print(feat_imprt)\n",
    "\n",
    "# useful features:\n",
    "print('useful features:')\n",
    "threshold = np.median(feat_imprt)\n",
    "for i in np.where(feat_imprt > threshold)[0]:\n",
    "    print(df.columns[2:][i])\n",
    "print('\\nuseless features:')\n",
    "for i in np.where(feat_imprt <= threshold)[0]:\n",
    "    print(df.columns[2:][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form validation period results\n",
    "val_results_df = results_list[0]['results_df']\n",
    "val_results_df = val_results_df.drop('y', axis=1)\n",
    "val_results_df = val_results_df.merge(df,\n",
    "                                      on=['date'],\n",
    "                                      how='inner', \n",
    "                                      validate='one_to_one')\n",
    "val_df = pd.DataFrame(data={'date': val_results_df.date.values,\n",
    "                            'y': val_results_df.y.values,\n",
    "                            'y_btc_eth_r_tp2_tp7': val_results_df['y_btc_eth_r_tp2_tp7'].values,\n",
    "                            'yhat': val_results_df.yhats.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very nice!\n",
    "val_df['yhat_decile'] = pd.qcut(val_df.yhat, q=10, labels=np.arange(10))\n",
    "val_df.groupby('yhat_decile')['y_btc_eth_r_tp2_tp7'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different trading rules\n",
    "trading_cost = 0.001\n",
    "\n",
    "max_return = 0\n",
    "\n",
    "# loop over leverage\n",
    "for leverage in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for long_threshold in range(10):\n",
    "        for short_threshold in range(10):\n",
    "            # grab yhats\n",
    "            yhats = val_df.yhat.values\n",
    "\n",
    "            # figure out thresholds\n",
    "            neg_yhats = yhats[yhats<0]\n",
    "            pos_yhats = yhats[yhats>=0]\n",
    "            neg_threshold = pd.qcut(neg_yhats, q=10).categories[short_threshold].right\n",
    "            pos_threshold = pd.qcut(pos_yhats, q=10).categories[long_threshold].left\n",
    "\n",
    "            # turn yhats into actual long or short position\n",
    "            pos = np.piecewise(yhats, [yhats<=neg_threshold, \n",
    "                                       (yhats>neg_threshold)&(yhats<=pos_threshold), \n",
    "                                       yhats>pos_threshold],\n",
    "                               [-1, 0, 1])\n",
    "\n",
    "            # figure out places to charge me\n",
    "            # by finding indices where previous value is different and the index value is not 0\n",
    "            trading_fee_array = np.concatenate((np.array([trading_cost]), \n",
    "                                               ((pos[1:]!=pos[:-1])&(pos[1:]!=0))*1*trading_cost))\n",
    "\n",
    "            # calc returns after cost\n",
    "            returns = pos*val_df.y_btc_eth_r_tp2_tp7*leverage\n",
    "            returns = returns - trading_fee_array\n",
    "\n",
    "            # report\n",
    "            overall_return = np.prod(returns+1)-1\n",
    "            if overall_return > max_return:\n",
    "                max_return = overall_return\n",
    "                \n",
    "                print('leverage '+str(leverage))\n",
    "                print('long thres '+str(long_threshold))\n",
    "                print('short threshold '+str(short_threshold))\n",
    "                \n",
    "                print('overal return '+str(np.round(overall_return, 4)))\n",
    "                start_index = 0\n",
    "                for i in range(int(2*24*30.5), 10272, int(2*24*30.5)):\n",
    "                    end_index = i\n",
    "                    month_returns = returns[start_index:end_index]\n",
    "                    month_returns = np.prod(month_returns+1)-1\n",
    "\n",
    "                    start_index = i\n",
    "                    print('months return ' +str(np.round(month_returns, 4)))\n",
    "\n",
    "                # report return adjusted by std of negative returns\n",
    "                std_neg_returns = np.std(returns[returns<0])\n",
    "                print('adjusted sharpe '+str(np.round(overall_return/std_neg_returns,4)))\n",
    "\n",
    "                # max dd\n",
    "                cumulative_ret=(returns+1).cumprod()\n",
    "                roll_max=cumulative_ret.rolling(len(cumulative_ret), min_periods=1).max()\n",
    "                dd=cumulative_ret/roll_max\n",
    "                max_dd=dd.min() - 1\n",
    "                print('max dd '+str(np.round(max_dd, 4)))\n",
    "\n",
    "                # sharpe\n",
    "                sharpe = np.mean(returns)/np.std(returns)*np.sqrt(365.25*24*2)\n",
    "                print('sharpe '+str(np.round(sharpe, 4)))\n",
    "                print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d43202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO THERE IS MORE TO GRAB FROM v-benchmark_gbdt_btceth_ls.ipynb as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in [2016, 2017, 2018, 2019, 2020, 2021]:\n",
    "#     lasso_df = df[df.date.dt.year == year].copy()\n",
    "\n",
    "#     # set lasso parameters\n",
    "#     rhs_feats = list(included_feats)\n",
    "#     lhs_col   = 'y_btc_eth_diff_r_tp5_tp370'\n",
    "#     alpha     = 0.001\n",
    "\n",
    "#     # scale RHS\n",
    "#     y = lasso_df[lhs_col].values\n",
    "#     X = lasso_df[rhs_feats].values\n",
    "#     X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "#     # fit\n",
    "#     model = Lasso(alpha=alpha, fit_intercept=False)\n",
    "#     model.fit(X_scaled, y)\n",
    "\n",
    "#     # show selected feats\n",
    "#     lasso_feats = [rhs_feats[i] for i in np.nonzero(model.coef_)[0]]\n",
    "#     print(year)\n",
    "#     print(lasso_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14903da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ONCE DONE WITH THIS; TURN INTO PYTHON SCRIPT\n",
    "# TODO MOVE ANY FUNCTIONS THAT I MAY USE ELSEWHERE TO COMMON FOLDER SHARED ACROSS PROJECTS THAT I JUST IMPORT WHEN NEEDED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
