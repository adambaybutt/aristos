{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e7ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0828b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGeomAvg(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the geometric average of a vector of simple returns.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar geometric average.\n",
    "    \"\"\"\n",
    "    geom_avg_at_given_freq = np.prod(1+returns)**(1/len(returns))-1\n",
    "    if annualized==False:\n",
    "        return geom_avg_at_given_freq\n",
    "    else:\n",
    "        return (geom_avg_at_given_freq+1)**periods_in_year-1\n",
    "\n",
    "def calcTSAvgReturn(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float:\n",
    "    \"\"\" Calculate the time series mean return of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar time series mean return.\n",
    "    \"\"\"\n",
    "    mean_ret_at_given_freq = np.mean(returns)\n",
    "    if annualized == False:\n",
    "        return mean_ret_at_given_freq\n",
    "    else:\n",
    "        mean_ret = periods_in_year*mean_ret_at_given_freq\n",
    "        if mean_ret < -1:\n",
    "            return -1.\n",
    "        else:\n",
    "            return mean_ret\n",
    "\n",
    "def calcTotalReturn(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float:\n",
    "    \"\"\" Calculate the total return of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar total return.\n",
    "    \"\"\"\n",
    "    total_return = np.prod(1+returns)-1\n",
    "    if annualized==False:\n",
    "        return total_return\n",
    "    else:\n",
    "        return (total_return+1)**(periods_in_year/len(returns))-1\n",
    "\n",
    "def calcSD(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the standard deviation of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar standard deviation.\n",
    "    \"\"\"\n",
    "    sd_at_given_freq = np.std(returns)\n",
    "    if annualized==False:\n",
    "        return sd_at_given_freq\n",
    "    else:\n",
    "        return np.sqrt(periods_in_year)*sd_at_given_freq\n",
    "\n",
    "def calcSharpe(returns: np.array,\n",
    "    periods_in_year: int,\n",
    "    risk_free_returns: np.array=None) -> float:\n",
    "    \"\"\" Calculate the annual Sharpe Ratio of a vector of simple returns. \n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "        risk_free_returns (np.array): vector of simple returns of the risk free rate.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar standard deviation.\n",
    "    \"\"\"\n",
    "    if risk_free_returns is not None:\n",
    "        returns = returns - risk_free_returns\n",
    "    \n",
    "    return (calcTSAvgReturn(returns, annualized=True, periods_in_year=periods_in_year) /\n",
    "            calcSD(returns, annualized=True, periods_in_year=periods_in_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd8e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcTransactionCosts(positions: np.array) -> np.array:\n",
    "    ''' Calculate a vector of transaction costs which are positive numbers in return units.\n",
    "    \n",
    "    Args:\n",
    "        positions (np.array): vector of positions, where positive is long and above 1, in absolute\n",
    "                              value terms, is a leveraged position.\n",
    "\n",
    "    Returns:\n",
    "        tc (np.array): vector of transaction costs in return terms.\n",
    "    '''\n",
    "    # transaction costs, in return terms, from kraken for trading two spots paris, on margin\n",
    "    tc_to_open        = 0.0015\n",
    "    tc_to_close       = 0.0015\n",
    "    tc_to_open_margin = 0.00015\n",
    "    tc_margin_12_hr    = 0.0003\n",
    "\n",
    "    # initial tc array\n",
    "    tc = np.zeros(len(positions))\n",
    "\n",
    "    # set first tc\n",
    "    first_position = positions[0]\n",
    "    if first_position == 0:\n",
    "        tc[0] = 0\n",
    "    elif (-1 <= first_position) & (first_position <= 1):\n",
    "        tc[0] = tc_to_open\n",
    "    elif (-5 <= first_position) & (first_position <= 5):\n",
    "        tc[0] = tc_to_open+tc_to_open_margin+tc_margin_12_hr\n",
    "    else:\n",
    "        raise ValueError('first position is not a valid position.')\n",
    "\n",
    "    # set remaining tc's\n",
    "    for i in range(1,len(tc)):\n",
    "        prev_position = positions[i-1]\n",
    "        current_position = positions[i]\n",
    "        if current_position == prev_position:\n",
    "            if np.abs(current_position)>1:\n",
    "                tc[i] = tc_margin_12_hr\n",
    "        else:\n",
    "            if current_position==0:\n",
    "                tc[i] = tc_to_close\n",
    "            elif (-1 <= current_position) & (current_position <= 1):\n",
    "                tc[i] = tc_to_close+tc_to_open\n",
    "            elif (-5 <= current_position) & (current_position <= 5):\n",
    "                tc[i] = tc_to_close+tc_to_open+tc_to_open_margin+tc_margin_12_hr\n",
    "            else: \n",
    "                raise ValueError('position '+str(i)+' is not a valid position.')\n",
    "\n",
    "    # adjust last tc element for closing position\n",
    "    last_position = positions[-1]\n",
    "    if np.abs(last_position)>0:\n",
    "        tc[-1] += tc_to_close\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0d8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitRF(train_df: pd.DataFrame,\n",
    "    lhs_col: str,\n",
    "    rhs_cols: list,\n",
    "    hps: dict) -> RandomForestClassifier: \n",
    "    ''' Fit random forest on given training data using RHS to predict LHS with given hps.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): training data.\n",
    "        lhs_col (str): LHS column to predict.\n",
    "        rhs_cols (list): list of strings of RHS features.\n",
    "        hps (dict): key value pairs for hyperparameters to set including:\n",
    "                    `num_estimators`, `subsample`, `min_samp_leaf`, `max_depth`, \n",
    "                    `max_feature`, `loss_func`, and `weight_func`.\n",
    "\n",
    "    Returns:\n",
    "        (RandomForestClassifier): fitted model.\n",
    "    '''\n",
    "    # obtain the rhs and lhs data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # build the model\n",
    "    model = RandomForestClassifier(n_estimators=hps['num_estimator'],\n",
    "        criterion=hps['loss_func'],\n",
    "        max_depth=hps['max_depth'],\n",
    "        min_samples_leaf=hps['min_samp_leaf'],\n",
    "        max_features=hps['max_feature'],\n",
    "        n_jobs=4,\n",
    "        random_state=int(hps['num_estimator']\n",
    "            *hps['subsample']\n",
    "            *hps['min_samp_leaf']\n",
    "            *hps['max_depth']\n",
    "            *hps['max_feature']),\n",
    "        class_weight=hps['class_balance'],\n",
    "        max_samples=hps['subsample'])\n",
    "\n",
    "    # calculate sample weights as linearly spaced from 0 to max value s.t. it sums to one\n",
    "    num_samples = train_lhs.shape[0]\n",
    "    if hps['weight_func'] == 'linear':\n",
    "        weights = np.arange(0, num_samples)\n",
    "        weights = weights/np.sum(weights)\n",
    "        epsilon = weights[1]/2\n",
    "        weights[0] = epsilon\n",
    "        weights[-1] -= epsilon\n",
    "    elif hps['weight_func'] == 'uniform':\n",
    "        weights = np.ones(num_samples)/num_samples\n",
    "    elif hps['weight_func'] == 'log':\n",
    "        weights = np.log(np.arange(1,num_samples+1))\n",
    "        weights = weights / np.sum(weights)\n",
    "    elif hps['weight_func'] == 'exp':\n",
    "        weights = np.arange(num_samples)**1.1\n",
    "        weights = weights / np.sum(weights)\n",
    "    else:\n",
    "        raise ValueError('the weights function must be set to linear, uniform, log, or exp.')\n",
    "\n",
    "    # fit\n",
    "    model.fit(X=train_rhs, \n",
    "        y=train_lhs, \n",
    "        sample_weight=weights)\n",
    "\n",
    "    # obtain training yhats and accuracy\n",
    "    train_yhats = model.predict(train_rhs)\n",
    "    train_acc = model.score(train_rhs, train_lhs)\n",
    "\n",
    "    return model, train_acc, train_yhats\n",
    "\n",
    "def genRFYhats(oos_df: pd.DataFrame, \n",
    "    model: RandomForestClassifier, \n",
    "    lhs_col: str, \n",
    "    rhs_cols: list) -> int: \n",
    "    \"\"\" Generate predicted probabilities for input data using a random forest classifier model.\n",
    "\n",
    "    Args:\n",
    "        oos_df (pd.DataFrame): out of sample data to fit on.\n",
    "        model (RandomForestClassifier): trained random forest model.\n",
    "        lhs_col (str): name of the column containing the label data.\n",
    "        rhs_cols (list): column names containing the feature data.\n",
    "\n",
    "    Returns:\n",
    "        (int): predicted scalar class.\n",
    "    \"\"\"\n",
    "    # obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    \n",
    "    # form the yhats\n",
    "    yhat = model.predict(oos_rhs)\n",
    "    \n",
    "    # Return results\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3471d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df: pd.DataFrame, \n",
    "    lhs_col: str,\n",
    "    val_start_date: str,\n",
    "    num_cpus: int,\n",
    "    arch_name: str, \n",
    "    out_csv_fp: str) -> list:\n",
    "    ''' run step-forward cross validation to select optimal hyperparameters for target model \n",
    "        returning fitting yhats and models as well as outputting results to csv.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): panel of training and val data with date index, LHS variable named \n",
    "                           `lhs_col`, and remaining cols are RHS features.\n",
    "        lhs_col (str): name of the lhs target column.\n",
    "        val_start_date (str): the first date for the validation period.\n",
    "        val_end_date (str): the last date for the validation period.\n",
    "        num_cpus (int): number of cpus to use when parallelizing.\n",
    "        arch_name (str): name of architecture to use when saving intermitent results.\n",
    "        out_csv_fp (str): filepath to output the csv file without `.csv' on end.\n",
    "          \n",
    "    Returns:\n",
    "        (list): list of dictionaries for each hyperparameter fit where each list contains keys of:\n",
    "                    `hps` is dict of hyperparameter combination,\n",
    "                    `yhats` is an array of validation period yhats, and,\n",
    "                    `models` is list of fitted models.  \n",
    "    '''\n",
    "    # initialize args\n",
    "    val_dates = np.unique(df[val_start_date:].index.values)\n",
    "    results_list = []\n",
    "    csv_dict_list = []\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # initialize hp grid for gbdt\n",
    "    # TODO: dial in return threshold to 3 options [0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.011, 0.012]\n",
    "    # TODO: dial in max depth to 3 options\n",
    "    # TODO: dial in num est to 3 options\n",
    "    # TODO: dial in max feats to 3 options [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "    # TODO: tune across num est, max depth, max feats, return threshold, and whether to balance classes\n",
    "    # TODO: dial in subsamples [0.9, 0.95, 0.99, 0.999]\n",
    "    # TODO dial in min sample leafs [5, 10, 20, 50, 100, 250, 500, 1000]\n",
    "    # TODO: dial in 3 options for all: num est, max depth, max feat, loss funcs, weight funcs, class balance, and ret threshold\n",
    "    #       loss funcs ['log_loss', 'gini', 'entropy'] and weight funs ['uniform', 'linear', 'log', 'exp'] \n",
    "    # TODO: combine all the results into csv \n",
    "    num_estimators  = [250]\n",
    "    subsamples      = [0.95]\n",
    "    min_samp_leafs  = [50]\n",
    "    max_depths      = [2, 3, 4, 5, 6, 7]\n",
    "    max_features    = [0.2]\n",
    "    loss_funcs      = ['log_loss']\n",
    "    weight_funcs    = ['linear']\n",
    "    class_balances  = ['balanced'] \n",
    "    ret_thresholds  = list(np.arange(1,21)/1000)\n",
    "\n",
    "    # loop over all hp combos\n",
    "    for hps in itertools.product(num_estimators,\n",
    "                                 subsamples,\n",
    "                                 min_samp_leafs,\n",
    "                                 max_depths, \n",
    "                                 max_features,\n",
    "                                 loss_funcs,\n",
    "                                 weight_funcs,\n",
    "                                 class_balances,\n",
    "                                 ret_thresholds):\n",
    "        # initialize args\n",
    "        results_dict = {}\n",
    "        results_dict['hps'] = {'num_estimator': hps[0],\n",
    "            'subsample': hps[1],\n",
    "            'min_samp_leaf': hps[2],\n",
    "            'max_depth': hps[3],\n",
    "            'max_feature': hps[4],\n",
    "            'loss_func': hps[5],\n",
    "            'weight_func': hps[6],\n",
    "            'class_balance': hps[7],\n",
    "            'ret_threshold': hps[8]}\n",
    "        print(results_dict['hps'], '\\n') # monitor progress\n",
    "\n",
    "        # form lhs\n",
    "        cv_df = df.copy()\n",
    "        ret_threshold = results_dict['hps']['ret_threshold']\n",
    "        cv_df['y'] = 1\n",
    "        cv_df.loc[cv_df[lhs_col] > ret_threshold, 'y'] = 2\n",
    "        cv_df.loc[cv_df[lhs_col] < -ret_threshold, 'y'] = 0\n",
    "\n",
    "        # remove actual returns and save val period returns\n",
    "        btc_eth_diff = cv_df[val_start_date:][lhs_col].values\n",
    "        cv_df = cv_df.drop(lhs_col, axis=1)\n",
    "\n",
    "        # fit model on all val dates\n",
    "        tic = time.perf_counter()\n",
    "        def loopOverValDates(val_date): # set up as a func to loop over\n",
    "            # form train and val data\n",
    "            train_df = cv_df[cv_df.index < val_date].copy()\n",
    "            val_df   = cv_df[cv_df.index == val_date].copy()\n",
    "\n",
    "            # fit model and generate yhats for val week\n",
    "            model, train_acc, train_yhats = fitRF(train_df, 'y', rhs_cols, hps=results_dict['hps'])\n",
    "            yhats = genRFYhats(val_df, model, 'y', rhs_cols)\n",
    "\n",
    "            return yhats, model, train_acc, train_yhats\n",
    "\n",
    "        val_results = Parallel(n_jobs=int(num_cpus/4))(delayed(loopOverValDates)(val_date) for val_date in tqdm(val_dates))\n",
    "\n",
    "        # extract validation periods results\n",
    "        yhats_list = []\n",
    "        models_list = []\n",
    "        train_acc_list = []\n",
    "        train_yhats_list = []\n",
    "        for t in range(len(val_results)):\n",
    "            yhats_list.append(val_results[t][0])\n",
    "            models_list.append(val_results[t][1])\n",
    "            train_acc_list.append(val_results[t][2])\n",
    "            train_yhats_list.append(val_results[t][3])\n",
    "        yhats = np.array(yhats_list)-1\n",
    "        ys    = cv_df[val_start_date:].y.values-1\n",
    "        results_dict['yhats'] = yhats\n",
    "        results_dict['models'] = models_list\n",
    "\n",
    "        # save results to master result list    \n",
    "        results_list.append(results_dict)\n",
    "\n",
    "        # save results to csv to monitor during cv\n",
    "        toc = time.perf_counter()\n",
    "        csv_dict = results_dict['hps'].copy()\n",
    "        del results_dict\n",
    "        csv_dict['arch_name'] = arch_name\n",
    "        csv_dict['val_start_date'] = val_start_date\n",
    "        csv_dict['val_end_date'] = np.datetime_as_string(np.max(df.index.values))[:10]\n",
    "        csv_dict['runtime_mins'] = round((toc - tic)/60, 0) \n",
    "        total_obs = 0\n",
    "        total_yhat_long = 0\n",
    "        total_yhat_short = 0\n",
    "        for train_yhats in train_yhats_list:\n",
    "            total_obs += train_yhats.shape[0]\n",
    "            total_yhat_long += np.sum(train_yhats==1)\n",
    "            total_yhat_short += np.sum(train_yhats==-1)\n",
    "        csv_dict['yhat_long_pct_train'] = total_yhat_long / total_obs\n",
    "        csv_dict['yhat_short_pct_train'] = total_yhat_short / total_obs\n",
    "        csv_dict['accuracy_train'] = np.mean(np.array(train_acc_list))\n",
    "        csv_dict['yhat_long_pct'] = np.sum(yhats==1)/len(yhats)\n",
    "        csv_dict['yhat_short_pct'] = np.sum(yhats==-1)/len(yhats)\n",
    "        csv_dict['accuracy'] = np.sum(yhats == ys)/len(ys)\n",
    "        tc = calcTransactionCosts(yhats)\n",
    "        returns = yhats*btc_eth_diff - tc\n",
    "        csv_dict['geom_mean'] = calcGeomAvg(returns)\n",
    "        csv_dict['sharpe'] = calcSharpe(returns, periods_in_year=365*2)\n",
    "        csv_dict['sd_ann'] = calcSD(returns, annualized=True, periods_in_year=365*2)\n",
    "        csv_dict['ts_mean_ann'] = calcTSAvgReturn(returns, annualized=True, periods_in_year=365*2)\n",
    "        for lev in [2, 3, 4, 5]:\n",
    "            lev_yhats = yhats*lev\n",
    "            lev_tc = calcTransactionCosts(lev_yhats)\n",
    "            lev_returns = lev_yhats*btc_eth_diff - lev_tc \n",
    "            csv_dict['geom_mean_x'+str(lev)] = calcGeomAvg(lev_returns)\n",
    "            csv_dict['sharpe_x'+str(lev)] = calcSharpe(lev_returns, periods_in_year=365*2)\n",
    "            csv_dict['sd_ann_x'+str(lev)] = calcSD(lev_returns, annualized=True, periods_in_year=365*2)\n",
    "            csv_dict['ts_mean_ann_x'+str(lev)] = calcTSAvgReturn(lev_returns, annualized=True, periods_in_year=365*2)\n",
    "        csv_dict_list.append(csv_dict)\n",
    "        results_df = pd.DataFrame(csv_dict_list)\n",
    "\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = out_csv_fp+'_'+arch_name+'_'+timestr+'.csv'\n",
    "        results_df.to_csv(fp, index=False)\n",
    "\n",
    "        # output results to track\n",
    "        print(csv_dict['runtime_mins'])\n",
    "        print('accuracy:'+str(csv_dict['accuracy']))\n",
    "        print('\\n\\n\\n')\n",
    "        gc.collect()\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "349eb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotYvsYhat(y: np.array, yhats: np.array, num_bins: int=10):\n",
    "    ''' plot average values of y against bins of yhat.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): vector of target variable.\n",
    "        yhats (np.array): vector of fitted values.\n",
    "        num_bins (int): number of bins to group the yhats into.\n",
    "    '''\n",
    "    # form data\n",
    "    temp_df = pd.DataFrame(data={'y': y, 'yhat': yhats})\n",
    "    temp_df['yhat_bin'] = pd.qcut(temp_df.yhat, q=num_bins, labels=np.arange(num_bins))\n",
    "\n",
    "    # plot\n",
    "    temp_df.groupby('yhat_bin')['y'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "307126c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_estimator': 250, 'subsample': 0.95, 'min_samp_leaf': 50, 'max_depth': 2, 'max_feature': 0.2, 'loss_func': 'log_loss', 'weight_func': 'linear', 'class_balance': 'balanced', 'ret_threshold': 0.001} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 220/548 [01:34<02:23,  2.29it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    in_fp = '../1-data/clean/bars_btceth_12hour.pkl'\n",
    "    cv_out_fp = '../3-output/cv_results'\n",
    "\n",
    "    # read in and prep training+validation data\n",
    "    df = pd.read_pickle(in_fp)\n",
    "    df = df.set_index('date')\n",
    "    df = df.astype('float32')\n",
    "    df = df[:'2022-03-31']\n",
    "    \n",
    "    # run the cv\n",
    "    cv_results = runCV(df, \n",
    "        lhs_col='y_btc_eth_diff_r_tp5_tp730',\n",
    "        val_start_date='2021-07-01',\n",
    "        num_cpus=20, \n",
    "        arch_name='rf_multiclass', \n",
    "        out_csv_fp=cv_out_fp)\n",
    "\n",
    "    # select best model\n",
    "    # TODO SELECT THE BEST MODEL\n",
    "\n",
    "    # report y's vs yhat's distribution\n",
    "    # TODO plot avg return by the three bins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60157e29",
   "metadata": {},
   "source": [
    "##### Backtest:\n",
    "    - fit GBDT predicting long or short\n",
    "    - execute an expanding window CV across all GBDT hyperparameters\n",
    "        - initialize training data as 2016 thru Q2 2021 and validation data is Q3 2021 through Q1 2022\n",
    "        - fit on training data and predict on first observation of validation data\n",
    "        - reset training data to include the next row from the validation data and re-fit+predict.\n",
    "        - generate yhats for the entire validation period.\n",
    "    - fit lasso in sample way across 10 different lambdas, take lasso that performs best, fit linear reg on those feats as a benchmark for GBDT to out perform; use lin reg of gbdt dont outperform dis.\n",
    "    - portfolio optimization: \n",
    "        - ex post take all the yhats to form a trading rule mapping from this vector of yhats to vector of 5,4,3,2,1,0,-1,-2,-3,-4,-5 for 5x long to 5x short. \n",
    "        - so basically where do i draw these 10 lines in my dist of 0 to 1? \n",
    "        - max returns or max sharpe ratio by picking these 10 points, monotoically, between 0 and 1 to create vector of positions. \n",
    "        - adjust all of this to take out transaction costs both open+close costs as well as margin costs. maybe these two costs are just vectors as well.\n",
    "    - calc avg return at 6 hour freq, total return over val period, geom avg return annualized, and sd of simple returns annualized.\n",
    "    - select highest geom avg return and also one with highest sharpe\n",
    "    - confirm both have geom avg return statistically different from zero.\n",
    "        - bootstrap some portfolio return s.e. to calc t stat for backtest return to see if stat sig diff from zero. \n",
    "        - given my benchmark is zero. \n",
    "        - ensure my validation period is big enough that i could get sig result, i.e. small enough S.E. just randomly generate 10k vectors of positions to calc geom average return; form empirical dist; take standard deviation of this dist to get the s.e. \n",
    "        - adjust all of this to take out transaction costs\n",
    "    - for selected model, go fit in true out of sample period of Q2-Q4 2022 to confirm again geom avg return is stat different from zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "978fb2ce",
   "metadata": {},
   "source": [
    "All of the below code is old code that I will delete once I have completed this backtest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO SOME BOOTSTRAP CODE TO USE LATER\n",
    "\n",
    "# generate 100 vectors of length 1096 selecting from [-5,-4,-3,-2,-1,0,1,2,3,4,5]\n",
    "bs_size = 100\n",
    "periods = 1096\n",
    "positions = [-5,-4,-3,-2,-1,-0.75,-0.5,-0.25,0,0.25,0.5,0.75,1,2,3,4,5]\n",
    "portfolios_list = []\n",
    "for i in range(bs_size):\n",
    "    np.random.seed(i)\n",
    "    portfolios_list.append(np.random.choice(positions, size=periods))\n",
    "\n",
    "# calc returns\n",
    "btc_eth_diff_array = df[(df.date >= '2021-07-01') & (df.date < '2022-04-01')].y_btc_eth_diff_r_tp5_tp370.values\n",
    "returns_list = []\n",
    "for i in range(bs_size):\n",
    "    portfolio = portfolios_list[i]\n",
    "    geom_return = np.prod(1+portfolio*btc_eth_diff_array)**(1/periods)-1\n",
    "    returns_list.append(geom_return)\n",
    "# calc s.e. of dist\n",
    "# think thru how to confirm this is enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6359323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207651c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitOLS(train_df, lhs_col, rhs_cols):\n",
    "    # Obtain the training input and output data and, if passed, validation data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "\n",
    "    # add constant\n",
    "    train_rhs = np.concatenate((np.ones([len(train_lhs),1]), train_rhs),\n",
    "                               axis=1)\n",
    "\n",
    "    # fit\n",
    "    betahat = np.matmul(np.linalg.inv(np.matmul(train_rhs.T, train_rhs)),\n",
    "                        np.matmul(train_rhs.T, train_lhs))\n",
    "\n",
    "    return betahat\n",
    "\n",
    "\n",
    "def genOLSYhats(oos_df, model, lhs_col, rhs_cols):    \n",
    "    # Obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    oos_lhs   = oos_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # add constant\n",
    "    oos_rhs   = np.concatenate((np.ones([len(oos_lhs),1]), oos_rhs),\n",
    "                                axis=1).reshape(-1)\n",
    "    \n",
    "    # Form the yhats\n",
    "    yhats = np.dot(model, oos_rhs)\n",
    "    \n",
    "    # Return results\n",
    "    return pd.DataFrame(data={'date': oos_df.index.values,\n",
    "                              'y': np.array(oos_df['y'].values),\n",
    "                              'yhats': yhats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e614cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLasso(train_df, hps_yhats_dict, lhs_col, rhs_cols):\n",
    "    # Extract hps\n",
    "    alpha = hps_yhats_dict['alpha']\n",
    "\n",
    "    # Obtain the training input and output data and, if passed, validation data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "\n",
    "    # standardize\n",
    "    scaler = StandardScaler()\n",
    "    train_rhs = scaler.fit_transform(train_rhs)\n",
    "\n",
    "    # fit\n",
    "    model = linear_model.Lasso(alpha=alpha, \n",
    "                               selection='cyclic')\n",
    "    model.fit(train_rhs, train_lhs)\n",
    "\n",
    "    # gather what coefs were used\n",
    "    used_coefs = (model.coef_!=0)*1\n",
    "\n",
    "    return model, used_coefs, scaler\n",
    "\n",
    "def genLassoYhats(oos_df, model, lhs_col, rhs_cols, scaler):    \n",
    "    # Obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    oos_lhs   = oos_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # Standarize\n",
    "    oos_rhs   = scaler.transform(oos_rhs)\n",
    "    \n",
    "    # Form the yhats\n",
    "    yhats = model.predict(oos_rhs)\n",
    "    \n",
    "    # Return results\n",
    "    return pd.DataFrame(data={'date': oos_df.index.values,\n",
    "                              'y': np.array(oos_df['y'].values),\n",
    "                              'yhats': yhats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4558459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelPortfolioWeights(df):\n",
    "    # set parameters for tertile weights\n",
    "    # note: can't go to zero on any as there may be week where that is only yhat\n",
    "    bottom_tertile = 1/6\n",
    "    mid_tertile = 1/3\n",
    "    top_tertile = (1-bottom_tertile-mid_tertile)\n",
    "\n",
    "    # create portfolio weights\n",
    "    df = df.sort_values(by=['date', 'yhat'])\n",
    "    df['counts']  = 1\n",
    "    df['total_assets_per_week_tertile'] = df.groupby(['date', 'yhat']).counts.transform('sum')\n",
    "    df.loc[df.yhat == -1, 'prtfl_wght'] = bottom_tertile / df[df.yhat == -1].total_assets_per_week_tertile\n",
    "    df.loc[df.yhat == 0,  'prtfl_wght'] = mid_tertile / df[df.yhat == 0].total_assets_per_week_tertile\n",
    "    df.loc[df.yhat == 1,  'prtfl_wght'] = top_tertile / df[df.yhat == 1].total_assets_per_week_tertile\n",
    "\n",
    "    # clean up\n",
    "    df = df.drop(['counts', 'total_assets_per_week_tertile'], axis=1)\n",
    "\n",
    "    # fix weeks where portfolio weights add up to less than 1\n",
    "    temp_df = df.groupby(['date'])[['prtfl_wght']].sum()\n",
    "    temp_df = temp_df[~np.isclose(temp_df.prtfl_wght, 1)]\n",
    "    if 0<temp_df.shape[0]:\n",
    "        for i in range(temp_df.shape[0]):\n",
    "            date = temp_df.index.values[i]\n",
    "            total_weight = temp_df.prtfl_wght.values[i]\n",
    "            df.loc[df.index == date, 'prtfl_wght'] = df[df.index==date].prtfl_wght * (1/total_weight)\n",
    "            \n",
    "    # confirm portfolio weights roughly sum to 1 for each week\n",
    "    assert(len(np.unique(df.index)) == \n",
    "           np.sum(np.isclose(df.groupby(['date']).prtfl_wght.sum(), 1,\n",
    "                             rtol=1e-2, atol=1e-2)))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAnnualTransactionCosts(df):\n",
    "    # merge on the previous week's holdings to the new holdings\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df[temp_df.index<np.max(temp_df.index)]\n",
    "    temp_df.index = temp_df.index+np.timedelta64(1, 'D')\n",
    "    temp_df = temp_df[['asset', 'prtfl_wght']]\n",
    "    temp_df = temp_df.rename(columns={'prtfl_wght': 'prtfl_wght_tm7'})\n",
    "    df      = df.merge(temp_df,\n",
    "                       on=['date', 'asset'],\n",
    "                       how='outer',\n",
    "                       validate='one_to_one')\n",
    "\n",
    "    # calc weekly turnover and ensure it has the appropriate range\n",
    "    df['asset_to'] = np.abs(df.prtfl_wght - df.prtfl_wght_tm7)\n",
    "    to_df = df.groupby('date')[['asset_to']].sum().reset_index()\n",
    "    assert((np.min(to_df.asset_to)>=0) & (np.max(to_df.asset_to<=2)))\n",
    "\n",
    "    # correct the first and last week valid for buying the initial port and liquidating\n",
    "    to_df.loc[0, 'asset_to'] = 1\n",
    "    to_df = pd.concat((to_df, pd.DataFrame(data={'date': np.max(temp_df.index.values)+np.timedelta64(1, 'D'),\n",
    "                                                 'asset_to': 1}, index=[0])))\n",
    "    to_df = to_df.reset_index(drop=True)\n",
    "\n",
    "    # add transaction costs assuming maker and taker fee of 20 bps each\n",
    "    to_df['tc'] = to_df.asset_to*0.002\n",
    "\n",
    "    # return annualize transaction cost\n",
    "    return -np.sum(to_df.tc)/(to_df.shape[0]/52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983905fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_draw_down(r_df):\n",
    "    cumulative_ret=(r_df.r+1).cumprod()\n",
    "    roll_max=cumulative_ret.rolling(len(cumulative_ret), min_periods=1).max()\n",
    "    daily_drawdown=cumulative_ret/roll_max\n",
    "    max_daily_drawdown=daily_drawdown.min() - 1\n",
    "    return max_daily_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a85a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_1_week_loss(r_df):\n",
    "    max_loss=(r_df['r']+1).rolling(7).apply(np.prod)\n",
    "    max_loss_minus=max_loss.min()-1\n",
    "    return max_loss_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTestYhats(df, opt_hps, test_year=2022): \n",
    "    test_dates = np.unique(df[df.index.year == test_year].index.values)\n",
    "\n",
    "    all_cols = list(df.columns.values)\n",
    "    lhs_col  = ['y']\n",
    "    all_cols.remove('y')\n",
    "    rhs_cols = all_cols\n",
    "\n",
    "    def loopOverOOSDates(test_date):\n",
    "        # build data\n",
    "        temp_df       = df[df.index <= test_date].copy()\n",
    "        train_df      = temp_df[temp_df.index < test_date]\n",
    "        oos_df        = temp_df[temp_df.index == test_date]\n",
    "\n",
    "        # fit and predict\n",
    "        model          = fitGBDT(train_df, opt_hps, lhs_col, rhs_cols)\n",
    "        oos_results_df = genYhats(oos_df, model, lhs_col, rhs_cols)\n",
    "\n",
    "        return oos_results_df\n",
    "\n",
    "    # run\n",
    "    oos_results = Parallel(n_jobs=num_cpus)(delayed(loopOverOOSDates)(test_date) for test_date in tqdm(test_dates))\n",
    "\n",
    "    # Extract oos results\n",
    "    oos_results_df = pd.concat(oos_results)\n",
    "\n",
    "    return oos_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE CODE\n",
    "\n",
    "# Load in the data\n",
    "input_fp = '../3-data/clean/panel_btceth_30min.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "\n",
    "# Clean up the data \n",
    "df = df.set_index('date')\n",
    "df = df.astype('float32')\n",
    "\n",
    "# Select useful variables accoridng to Lasso\n",
    "df = df[['y',\n",
    "         'covar_btc_volume_sum_tm288',\n",
    "         'covar_btc_r_tm36',\n",
    "         'covar_btc_ema_144_t',\n",
    "         'covar_btc_ema_288_t',\n",
    "         'covar_eth_r_tm12',\n",
    "         'covar_eth_ma_24_t',\n",
    "         'covar_btc_rsi_tm288',\n",
    "         'covar_eth_rsi_tm2016',\n",
    "         'covar_eth_rsi_tm4032',\n",
    "         'macro_mcap_ret_ath_t',\n",
    "         'macro_mcap_altcoin_ret_ath_t']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV\n",
    "results_list = runCV(df, last_train_year=2019, val_end_year=2021,\n",
    "                     arch_name='lasso_longshort', num_cpus=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # step 1\n",
    "\n",
    "    # step 2\n",
    "\n",
    "    # step 3\n",
    "\n",
    "    # do gbdt with the full set where i randomly sample sqrt of them and then try with the 36 cols from lasso where i sample half or something\n",
    "    \n",
    "    # TODO explore if normalization helps or not and make sure no forward looking bias\n",
    "# played with the lasso code below to select a final set of 36 features\n",
    "    lasso_feats =  ['covar_btc_covar_trades_t_ma_tm8640',\n",
    "                    'covar_btc_covar_trades_t_min_tm30',\n",
    "                    'covar_btc_covar_trades_t_min_tm60',\n",
    "                    'covar_btc_covar_volume_t_min_tm4320',\n",
    "                    'covar_btc_p_log_t',\n",
    "                    'covar_btc_r_1min_ema_tm360',\n",
    "                    'covar_btc_r_5min_skew_tm720',\n",
    "                    'covar_btc_r_tm1440',\n",
    "                    'covar_btc_r_tm15',\n",
    "                    'covar_btc_r_tm86400',\n",
    "                    'covar_btc_rsi_tm720',\n",
    "                    'covar_eth_covar_trades_t_max_tm4320',\n",
    "                    'covar_eth_covar_trades_t_min_tm20',\n",
    "                    'covar_eth_covar_trades_t_vol_tm60',\n",
    "                    'covar_eth_covar_volume_t_ma_tm60',\n",
    "                    'covar_eth_covar_volume_t_ma_tm8640',\n",
    "                    'covar_eth_covar_volume_t_max_tm4320',\n",
    "                    'covar_eth_covar_volume_t_max_tm720',\n",
    "                    'covar_eth_covar_volume_t_sum_tm20',\n",
    "                    'covar_eth_covar_volume_t_sum_tm60',\n",
    "                    'covar_eth_covar_volume_t_vol_tm360',\n",
    "                    'covar_eth_p_t',\n",
    "                    'covar_eth_r_1min_ema_tm60',\n",
    "                    'covar_eth_r_1min_ma_tm360',\n",
    "                    'covar_eth_r_1min_vol_tm20',\n",
    "                    'covar_eth_r_1min_vol_tm5',\n",
    "                    'covar_eth_r_5min_ma_tm20160',\n",
    "                    'covar_eth_r_5min_ma_tm60',\n",
    "                    'covar_eth_r_5min_min_tm30',\n",
    "                    'covar_eth_r_5min_min_tm720',\n",
    "                    'covar_eth_r_cummax_t',\n",
    "                    'covar_eth_r_cummin_t',\n",
    "                    'covar_eth_r_tm120',\n",
    "                    'covar_eth_r_tm1440',\n",
    "                    'covar_eth_r_tm40320',\n",
    "                    'covar_eth_volume_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_master = results_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ca09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE RESULTS\n",
    "# with open('../3-data/derived/validation_2021_results_btceth_ols.pickle', 'wb') as handle:\n",
    "#     pickle.dump(results_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa665ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study for signal\n",
    "for i in [6, 5, 4, 3, 2, 1, 0]:\n",
    "    print(i)\n",
    "    df             = pd.read_pickle(input_fp)\n",
    "    val_results_df = results_list[i]['results_df']\n",
    "    val_results_df = val_results_df.drop('y', axis=1)\n",
    "    val_results_df = val_results_df.merge(df,\n",
    "                                          on=['date'],\n",
    "                                          how='inner', \n",
    "                                          validate='one_to_one')\n",
    "    val_df = pd.DataFrame(data={'date': val_results_df.date.values,\n",
    "                                'y': val_results_df.y.values,\n",
    "                                'y_btc_eth_r_tp2_tp7': val_results_df['y_btc_eth_r_tp2_tp7'].values,\n",
    "                                'yhat': val_results_df.yhats.values})\n",
    "\n",
    "    val_df['yhat'] = (val_df.yhat-np.min(val_df.yhat))\n",
    "    val_df['yhat'] = val_df.yhat/np.max(val_df.yhat)\n",
    "    for lev in [2, 3, 4, 5]:\n",
    "        for fee in [1e-4, 2e-4, 5e-4]:\n",
    "            print(lev)\n",
    "            print(fee)\n",
    "            yhat_vals = val_df.yhat.values\n",
    "            for yhat_val in yhat_vals:\n",
    "                val_df.loc[val_df.yhat<=yhat_val, 'pw'] = -1\n",
    "                val_df.loc[val_df.yhat>yhat_val, 'pw'] = 1\n",
    "                assert(0==val_df.pw.isnull().sum())\n",
    "                port_ret = np.prod(lev*(1+val_df.pw*val_df.y_btc_eth_r_tp2_tp7)-fee)**(1/len(val_df))-1\n",
    "                val_df.loc[val_df.yhat==yhat_val, 'ret'] = port_ret\n",
    "\n",
    "            val_df = val_df.drop('pw', axis=1)\n",
    "\n",
    "            val_df = val_df.sort_values(by='yhat')\n",
    "            val_df['yhat'] = np.arange(len(val_df))/len(val_df)\n",
    "            plt.plot(val_df.yhat, val_df.ret)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345dcfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10\n",
    "val_df['yhat_decile'] = pd.qcut(val_df.yhat, q=bins, labels=np.arange(bins))\n",
    "val_df.groupby('yhat_decile')['y_btc_eth_r_tp2_tp7'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study feature importance\n",
    "\n",
    "feat_imprt = np.zeros(df.shape[1]-2)\n",
    "models = results_list[0]['models']\n",
    "for model in models:\n",
    "    feat_imprt += model.feature_importances_\n",
    "feat_imprt = feat_imprt/(len(models)-1)\n",
    "print(feat_imprt)\n",
    "\n",
    "# useful features:\n",
    "print('useful features:')\n",
    "threshold = np.median(feat_imprt)\n",
    "for i in np.where(feat_imprt > threshold)[0]:\n",
    "    print(df.columns[2:][i])\n",
    "print('\\nuseless features:')\n",
    "for i in np.where(feat_imprt <= threshold)[0]:\n",
    "    print(df.columns[2:][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form validation period results\n",
    "val_results_df = results_list[0]['results_df']\n",
    "val_results_df = val_results_df.drop('y', axis=1)\n",
    "val_results_df = val_results_df.merge(df,\n",
    "                                      on=['date'],\n",
    "                                      how='inner', \n",
    "                                      validate='one_to_one')\n",
    "val_df = pd.DataFrame(data={'date': val_results_df.date.values,\n",
    "                            'y': val_results_df.y.values,\n",
    "                            'y_btc_eth_r_tp2_tp7': val_results_df['y_btc_eth_r_tp2_tp7'].values,\n",
    "                            'yhat': val_results_df.yhats.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very nice!\n",
    "val_df['yhat_decile'] = pd.qcut(val_df.yhat, q=10, labels=np.arange(10))\n",
    "val_df.groupby('yhat_decile')['y_btc_eth_r_tp2_tp7'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different trading rules\n",
    "trading_cost = 0.001\n",
    "\n",
    "max_return = 0\n",
    "\n",
    "# loop over leverage\n",
    "for leverage in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for long_threshold in range(10):\n",
    "        for short_threshold in range(10):\n",
    "            # grab yhats\n",
    "            yhats = val_df.yhat.values\n",
    "\n",
    "            # figure out thresholds\n",
    "            neg_yhats = yhats[yhats<0]\n",
    "            pos_yhats = yhats[yhats>=0]\n",
    "            neg_threshold = pd.qcut(neg_yhats, q=10).categories[short_threshold].right\n",
    "            pos_threshold = pd.qcut(pos_yhats, q=10).categories[long_threshold].left\n",
    "\n",
    "            # turn yhats into actual long or short position\n",
    "            pos = np.piecewise(yhats, [yhats<=neg_threshold, \n",
    "                                       (yhats>neg_threshold)&(yhats<=pos_threshold), \n",
    "                                       yhats>pos_threshold],\n",
    "                               [-1, 0, 1])\n",
    "\n",
    "            # figure out places to charge me\n",
    "            # by finding indices where previous value is different and the index value is not 0\n",
    "            trading_fee_array = np.concatenate((np.array([trading_cost]), \n",
    "                                               ((pos[1:]!=pos[:-1])&(pos[1:]!=0))*1*trading_cost))\n",
    "\n",
    "            # calc returns after cost\n",
    "            returns = pos*val_df.y_btc_eth_r_tp2_tp7*leverage\n",
    "            returns = returns - trading_fee_array\n",
    "\n",
    "            # report\n",
    "            overall_return = np.prod(returns+1)-1\n",
    "            if overall_return > max_return:\n",
    "                max_return = overall_return\n",
    "                \n",
    "                print('leverage '+str(leverage))\n",
    "                print('long thres '+str(long_threshold))\n",
    "                print('short threshold '+str(short_threshold))\n",
    "                \n",
    "                print('overal return '+str(np.round(overall_return, 4)))\n",
    "                start_index = 0\n",
    "                for i in range(int(2*24*30.5), 10272, int(2*24*30.5)):\n",
    "                    end_index = i\n",
    "                    month_returns = returns[start_index:end_index]\n",
    "                    month_returns = np.prod(month_returns+1)-1\n",
    "\n",
    "                    start_index = i\n",
    "                    print('months return ' +str(np.round(month_returns, 4)))\n",
    "\n",
    "                # report return adjusted by std of negative returns\n",
    "                std_neg_returns = np.std(returns[returns<0])\n",
    "                print('adjusted sharpe '+str(np.round(overall_return/std_neg_returns,4)))\n",
    "\n",
    "                # max dd\n",
    "                cumulative_ret=(returns+1).cumprod()\n",
    "                roll_max=cumulative_ret.rolling(len(cumulative_ret), min_periods=1).max()\n",
    "                dd=cumulative_ret/roll_max\n",
    "                max_dd=dd.min() - 1\n",
    "                print('max dd '+str(np.round(max_dd, 4)))\n",
    "\n",
    "                # sharpe\n",
    "                sharpe = np.mean(returns)/np.std(returns)*np.sqrt(365.25*24*2)\n",
    "                print('sharpe '+str(np.round(sharpe, 4)))\n",
    "                print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8430c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "possible_trades = [5, 4, 3, 2, 1, 0.75, 0.5, 0.25, -0.25, -0.5, -0.75, -1, -2, -3, -4, -5]\n",
    "\n",
    "# initialize objects\n",
    "max_sharpe = 0\n",
    "optimal_cutoffs = list(np.zeros(len(possible_trades)))\n",
    "optimal_positions = np.zeros(len(y))\n",
    "\n",
    "# determine optimal cutoff for max long\n",
    "trade = possible_trades[0]\n",
    "possible_cutoffs = np.flip(np.sort(yhats))\n",
    "optimal_cutoffs[0] = np.max(possible_cutoffs)\n",
    "\n",
    "for cutoff in possible_cutoffs:\n",
    "    positions = np.where(yhats >= cutoff, trade, optimal_positions)\n",
    "    tcs = calcTransactionCosts(positions)\n",
    "    returns = positions*y-tcs\n",
    "    sharpe  = calcSharpe(returns, periods_in_year=365*4)\n",
    "    if sharpe > max_sharpe:\n",
    "        max_sharpe = sharpe\n",
    "        print(calcGeomAvg(returns, annualized=True, periods_in_year=365*4))\n",
    "        optimal_cutoffs[0] = cutoff\n",
    "\n",
    "# update optimal positions\n",
    "optimal_positions = np.where(yhats >= optimal_cutoffs[0], trade, optimal_positions)\n",
    "\n",
    "# determine optimal cutoffs for long positions\n",
    "for i in range(1,8):\n",
    "    trade = possible_trades[i]\n",
    "    prev_cutoff = optimal_cutoffs[i-1]\n",
    "    possible_cutoffs = np.flip(np.sort(yhats[yhats <= prev_cutoff]))\n",
    "    optimal_cutoffs[i] = np.max(possible_cutoffs)\n",
    "    for cutoff in possible_cutoffs:\n",
    "        positions = np.where((yhats < prev_cutoff) & (yhats >= cutoff), trade, optimal_positions)\n",
    "        tcs = calcTransactionCosts(positions)\n",
    "        returns = positions*y-tcs\n",
    "        sharpe  = calcSharpe(returns, periods_in_year=365*4)\n",
    "        if sharpe > max_sharpe:\n",
    "            max_sharpe = sharpe\n",
    "            print(calcGeomAvg(returns, annualized=True, periods_in_year=365*4))\n",
    "            optimal_cutoffs[i] = cutoff\n",
    "\n",
    "    # update optimal positions\n",
    "    optimal_positions = np.where((yhats < prev_cutoff) & (yhats >= optimal_cutoffs[i]), trade, optimal_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d43202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO THERE IS MORE TO GRAB FROM v-benchmark_gbdt_btceth_ls.ipynb as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in [2016, 2017, 2018, 2019, 2020, 2021]:\n",
    "#     lasso_df = df[df.date.dt.year == year].copy()\n",
    "\n",
    "#     # set lasso parameters\n",
    "#     rhs_feats = list(included_feats)\n",
    "#     lhs_col   = 'y_btc_eth_diff_r_tp5_tp370'\n",
    "#     alpha     = 0.001\n",
    "\n",
    "#     # scale RHS\n",
    "#     y = lasso_df[lhs_col].values\n",
    "#     X = lasso_df[rhs_feats].values\n",
    "#     X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "#     # fit\n",
    "#     model = Lasso(alpha=alpha, fit_intercept=False)\n",
    "#     model.fit(X_scaled, y)\n",
    "\n",
    "#     # show selected feats\n",
    "#     lasso_feats = [rhs_feats[i] for i in np.nonzero(model.coef_)[0]]\n",
    "#     print(year)\n",
    "#     print(lasso_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14903da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO MOVE ANY FUNCTIONS THAT I MAY USE ELSEWHERE TO COMMON FOLDER SHARED ACROSS PROJECTS THAT I JUST IMPORT WHEN NEEDED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
