{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e1e7ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bf0d8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitGBDT(train_df: pd.DataFrame,\n",
    "            lhs_col: str,\n",
    "            rhs_cols: list,\n",
    "            hps: dict) -> GradientBoostingClassifier: \n",
    "    ''' Fit GBDT on given training data using RHS columns to predict LHS column with given hps.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): training data.\n",
    "        lhs_col (str): LHS column to predict.\n",
    "        rhs_cols (list): list of strings of RHS features.\n",
    "        hps (dict): key value pairs for hyperparameters to set including: `learning_rate`, \n",
    "                    `num_estimators`, `subsample`, `min_samp_split`, `max_depth`, and `max_feature`.\n",
    "\n",
    "    Returns:\n",
    "        (GradientBoostingClassifier): fitted model.\n",
    "    '''\n",
    "    # obtain the rhs and lhs data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # build the model\n",
    "    model = GradientBoostingClassifier(loss='log_loss',\n",
    "        learning_rate=hps['learning_rate'],\n",
    "        n_estimators=hps['num_estimator'],\n",
    "        subsample=hps['subsample'],\n",
    "        min_samples_split=hps['min_samp_split'],\n",
    "        max_depth=hps['max_depth'],\n",
    "        max_features=hps['max_feature'],\n",
    "        verbose=0,\n",
    "        random_state=int(hps['learning_rate']\n",
    "            *hps['num_estimator']\n",
    "            *hps['subsample']\n",
    "            *hps['min_samp_split']\n",
    "            *hps['max_depth']\n",
    "            *hps['max_feature']))\n",
    "\n",
    "    # calculate sample weights as linearly spaced from 0 to max value s.t. it sums to one\n",
    "    num_samples = train_lhs.shape[0]\n",
    "    weights = np.arange(0, num_samples)\n",
    "    weights = weights/np.sum(weights)\n",
    "    epsilon = weights[1]/2\n",
    "    weights[0] = epsilon\n",
    "    weights[-1] -= epsilon\n",
    "\n",
    "    # fit\n",
    "    model.fit(X=train_rhs, \n",
    "        y=train_lhs, \n",
    "        sample_weight=weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def genGBDTYhats(oos_df: pd.DataFrame, \n",
    "    model: GradientBoostingClassifier, \n",
    "    lhs_col: str, \n",
    "    rhs_cols: list) -> np.array: \n",
    "    \"\"\" Generate predicted probabilities for input data using a gradient boosting classifier model.\n",
    "\n",
    "    Args:\n",
    "        oos_df (pd.DataFrame): out of sample data to fit on.\n",
    "        model (GradientBoostingClassifier): trained gradient Boosting model.\n",
    "        lhs_col (str): name of the column containing the label data.\n",
    "        rhs_cols (list): column names containing the feature data.\n",
    "\n",
    "    Returns:\n",
    "        (np.array)): array of predicted probabilities.\n",
    "    \"\"\"\n",
    "    # obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    \n",
    "    # form the yhats\n",
    "    yhats = model.predict_proba(oos_rhs)\n",
    "    \n",
    "    # Return results\n",
    "    return yhats[:,1] # second column corresponds to probability of label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3471d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df: pd.DataFrame, \n",
    "    val_start_date: str,\n",
    "    num_cpus: int,\n",
    "    arch_name: str, \n",
    "    out_csv_fp: str) -> list:\n",
    "    ''' run step-forward cross validation to select optimal hyperparameters for target model \n",
    "        returning fitting yhats and models as well as outputting results to csv.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): panel of training and val data with date index, LHS variable labeled `y`, \n",
    "                           and remaining cols are RHS features.\n",
    "        val_start_date (str): the first date for the validation period.\n",
    "        val_end_date (str): the last date for the validation period.\n",
    "        num_cpus (int): number of cpus to use when parallelizing.\n",
    "        arch_name (str): name of architecture to use when saving intermitent results.\n",
    "        out_csv_fp (str): filepath to output the csv file without `.csv' on end.\n",
    "          \n",
    "    Returns:\n",
    "        (list): list of dictionaries for each hyperparameter fit where each list contains keys of:\n",
    "                    `hps` is dict of hyperparameter combination,\n",
    "                    `yhats` is an array of validation period yhats, and,\n",
    "                    `models` is list of fitted models.  \n",
    "    '''\n",
    "    # initialize args\n",
    "    val_dates = np.unique(df[val_start_date:].index.values)\n",
    "    results_list = []\n",
    "    csv_dict_list = []\n",
    "    lhs_col  = 'y'\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # initialize hp grid for gbdt\n",
    "    learning_rates  = [5e-2, 1e-3, 1e-4] # TODO wide grid and then fine grid\n",
    "    num_estimators  = [100, 250]  # TODO do big grid # TODO do fine grid\n",
    "    subsamples      = [0.99] # TODO big grid and then fine grid\n",
    "    min_samp_splits = [0.001, 0.1,  0.5] \n",
    "    max_depths      = [2, 3, 4] # TODO do deeper\n",
    "    max_features    = [0.1, 0.2, 0.3] # TODO FINE GRID\n",
    "\n",
    "    # loop over all hp combos\n",
    "    for hps in itertools.product(learning_rates,\n",
    "                                 num_estimators,\n",
    "                                 subsamples,\n",
    "                                 min_samp_splits,\n",
    "                                 max_depths, \n",
    "                                 max_features):\n",
    "        # initialize args\n",
    "        results_dict = {}\n",
    "        results_dict['hps'] = {'learning_rate': hps[0],\n",
    "            'num_estimator': hps[1],\n",
    "            'subsample': hps[2],\n",
    "            'min_samp_split': hps[3],\n",
    "            'max_depth': hps[4],\n",
    "            'max_feature': hps[5]}\n",
    "        print(results_dict['hps'], '\\n') # monitor progress\n",
    "\n",
    "        # fit model on all val dates\n",
    "        tic = time.perf_counter()\n",
    "        def loopOverValDates(val_date): # set up as a func to loop over\n",
    "            # form train and val data\n",
    "            train_df = df[df.index < val_date].copy()\n",
    "            val_df   = df[df.index == val_date].copy()\n",
    "\n",
    "            # fit model and generate yhats for val week\n",
    "            model = fitGBDT(train_df, lhs_col, rhs_cols, hps=results_dict['hps'])\n",
    "            yhats = genGBDTYhats(val_df, model, lhs_col, rhs_cols)\n",
    "\n",
    "            return yhats, model\n",
    "        \n",
    "        val_results = Parallel(n_jobs=num_cpus)(delayed(loopOverValDates)(val_date) for val_date in tqdm(val_dates))\n",
    "\n",
    "        # extract validation periods results\n",
    "        yhats_list = []\n",
    "        models_list = []\n",
    "        for t in range(len(val_results)):\n",
    "            yhats_list.append(val_results[t][0])\n",
    "            models_list.append(val_results[t][1])\n",
    "        results_dict['yhats'] = np.array(yhats_list)\n",
    "        results_dict['models'] = models_list\n",
    "\n",
    "        # save results to master result list\n",
    "        results_list.append(results_dict)\n",
    "\n",
    "        # save results to csv to monitor during cv\n",
    "        toc = time.perf_counter()\n",
    "        \n",
    "        csv_dict = results_dict['hps'].copy()\n",
    "        csv_dict['arch_name'] = arch_name\n",
    "        csv_dict['val_start_date'] = val_start_date\n",
    "        csv_dict['val_end_date'] = np.datetime_as_string(np.max(df.index.values))[:10]\n",
    "        csv_dict['runtime_mins'] = round((toc - tic)/60, 0) \n",
    "        csv_dict['accuracy'] = np.sum(np.where(results_dict['yhats'] > 0.5, 1, 0).reshape(-1)\n",
    "            == df[val_start_date:].y.values)/df[val_start_date:].shape[0]\n",
    "        csv_dict_list.append(csv_dict)\n",
    "        cv_df = pd.DataFrame(csv_dict_list)\n",
    "\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = out_csv_fp+'_'+arch_name+'_'+timestr+'.csv'\n",
    "        cv_df.to_csv(fp, index=False)\n",
    "\n",
    "        # output results to track\n",
    "        print(csv_dict['runtime_mins'])\n",
    "        print('accuracy:'+str(csv_dict['accuracy']))\n",
    "        print('\\n\\n\\n')\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "307126c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.001, 'max_depth': 2, 'max_feature': 0.1} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "  9%|▉         | 99/1096 [18:12<3:03:20, 11.03s/it]\n",
      " 59%|█████▉    | 579/976 [08:20<05:43,  1.16it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 976/976 [02:54<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "accuracy:0.5235655737704918\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.001, 'max_depth': 2, 'max_feature': 0.2} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [05:36<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "accuracy:0.5184426229508197\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.001, 'max_depth': 2, 'max_feature': 0.3} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [08:20<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "accuracy:0.5225409836065574\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.001, 'max_depth': 3, 'max_feature': 0.1} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [04:09<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "accuracy:0.492827868852459\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.001, 'max_depth': 3, 'max_feature': 0.2} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [08:13<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "accuracy:0.5010245901639344\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.001, 'max_depth': 3, 'max_feature': 0.3} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [12:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "accuracy:0.5174180327868853\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.001, 'max_depth': 4, 'max_feature': 0.1} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [05:25<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "accuracy:0.5153688524590164\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.001, 'max_depth': 4, 'max_feature': 0.2} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [10:45<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "accuracy:0.507172131147541\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.001, 'max_depth': 4, 'max_feature': 0.3} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [16:03<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n",
      "accuracy:0.514344262295082\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.1, 'max_depth': 2, 'max_feature': 0.1} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [02:53<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "accuracy:0.5040983606557377\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.1, 'max_depth': 2, 'max_feature': 0.2} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [05:36<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "accuracy:0.5133196721311475\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.1, 'max_depth': 2, 'max_feature': 0.3} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [08:17<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "accuracy:0.492827868852459\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.1, 'max_depth': 3, 'max_feature': 0.1} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [04:07<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "accuracy:0.5297131147540983\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.1, 'max_depth': 3, 'max_feature': 0.2} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [08:05<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "accuracy:0.5225409836065574\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.1, 'max_depth': 3, 'max_feature': 0.3} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [12:05<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "accuracy:0.5235655737704918\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.1, 'max_depth': 4, 'max_feature': 0.1} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [05:14<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "accuracy:0.5010245901639344\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.1, 'max_depth': 4, 'max_feature': 0.2} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [10:28<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "accuracy:0.5112704918032787\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.1, 'max_depth': 4, 'max_feature': 0.3} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [15:35<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n",
      "accuracy:0.5102459016393442\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.5, 'max_depth': 2, 'max_feature': 0.1} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [02:42<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "accuracy:0.5153688524590164\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.5, 'max_depth': 2, 'max_feature': 0.2} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [05:15<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "accuracy:0.5194672131147541\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.5, 'max_depth': 2, 'max_feature': 0.3} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [07:51<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "accuracy:0.5133196721311475\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.5, 'max_depth': 3, 'max_feature': 0.1} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [03:37<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "accuracy:0.5163934426229508\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'learning_rate': 0.05, 'num_estimator': 100, 'subsample': 0.99, 'min_samp_split': 0.5, 'max_depth': 3, 'max_feature': 0.2} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 80/976 [00:25<05:15,  2.84it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    in_fp = '../1-data/clean/bars_btceth_6hour.pkl'\n",
    "    cv_out_fp = '../3-output/cv_results'\n",
    "\n",
    "    # read in and prep training+validation data\n",
    "    df = pd.read_pickle(in_fp)\n",
    "    df = df.set_index('date')\n",
    "    df = df.astype('float32')\n",
    "    df = df.drop('y_btc_eth_diff_r_tp5_tp370', axis=1)\n",
    "    df = df[:'2022-03-31']\n",
    "    \n",
    "    # run the cv\n",
    "    cv_results = runCV(df, \n",
    "        val_start_date='2021-07-31',\n",
    "        num_cpus=20, \n",
    "        arch_name='gbdt', \n",
    "        out_csv_fp=cv_out_fp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60157e29",
   "metadata": {},
   "source": [
    "##### Backtest:\n",
    "    - fit GBDT predicting long or short\n",
    "    - execute an expanding window CV across all GBDT hyperparameters\n",
    "        - initialize training data as 2016 thru Q2 2021 and validation data is Q3 2021 through Q1 2022\n",
    "        - fit on training data and predict on first observation of validation data\n",
    "        - reset training data to include the next row from the validation data and re-fit+predict.\n",
    "        - generate yhats for the entire validation period.\n",
    "    - fit lasso in sample way across 10 different lambdas, take lasso that performs best, fit linear reg on those feats as a benchmark for GBDT to out perform; use lin reg of gbdt dont outperform dis.\n",
    "    - portfolio optimization: \n",
    "        - ex post take all the yhats to form a trading rule mapping from this vector of yhats to vector of 5,4,3,2,1,0,-1,-2,-3,-4,-5 for 5x long to 5x short. \n",
    "        - so basically where do i draw these 10 lines in my dist of 0 to 1? \n",
    "        - max returns or max sharpe ratio by picking these 10 points, monotoically, between 0 and 1 to create vector of positions. \n",
    "        - adjust all of this to take out transaction costs both open+close costs as well as margin costs. maybe these two costs are just vectors as well.\n",
    "    - calc avg return at 6 hour freq, total return over val period, geom avg return annualized, and sd of simple returns annualized.\n",
    "    - select highest geom avg return and also one with highest sharpe\n",
    "    - confirm both have geom avg return statistically different from zero.\n",
    "        - bootstrap some portfolio return s.e. to calc t stat for backtest return to see if stat sig diff from zero. \n",
    "        - given my benchmark is zero. \n",
    "        - ensure my validation period is big enough that i could get sig result, i.e. small enough S.E. just randomly generate 10k vectors of positions to calc geom average return; form empirical dist; take standard deviation of this dist to get the s.e. \n",
    "        - adjust all of this to take out transaction costs\n",
    "    - for selected model, go fit in true out of sample period of Q2-Q4 2022 to confirm again geom avg return is stat different from zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "978fb2ce",
   "metadata": {},
   "source": [
    "All of the below code is old code that I will delete once I have completed this backtest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO SOME BOOTSTRAP CODE TO USE LATER\n",
    "\n",
    "# generate 100 vectors of length 1096 selecting from [-5,-4,-3,-2,-1,0,1,2,3,4,5]\n",
    "bs_size = 100\n",
    "periods = 1096\n",
    "positions = [-5,-4,-3,-2,-1,0,1,2,3,4,5]\n",
    "portfolios_list = []\n",
    "for i in range(bs_size):\n",
    "    np.random.seed(i)\n",
    "    portfolios_list.append(np.random.choice(positions, size=periods))\n",
    "\n",
    "# calc returns\n",
    "btc_eth_diff_array = df[(df.date >= '2021-07-01') & (df.date < '2022-04-01')].y_btc_eth_diff_r_tp5_tp370.values\n",
    "returns_list = []\n",
    "for i in range(bs_size):\n",
    "    portfolio = portfolios_list[i]\n",
    "    geom_return = np.prod(1+portfolio*btc_eth_diff_array)**(1/periods)-1\n",
    "    returns_list.append(geom_return)\n",
    "# calc s.e. of dist\n",
    "# think thru how to confirm this is enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6359323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207651c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitOLS(train_df, lhs_col, rhs_cols):\n",
    "    # Obtain the training input and output data and, if passed, validation data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "\n",
    "    # add constant\n",
    "    train_rhs = np.concatenate((np.ones([len(train_lhs),1]), train_rhs),\n",
    "                               axis=1)\n",
    "\n",
    "    # fit\n",
    "    betahat = np.matmul(np.linalg.inv(np.matmul(train_rhs.T, train_rhs)),\n",
    "                        np.matmul(train_rhs.T, train_lhs))\n",
    "\n",
    "    return betahat\n",
    "\n",
    "\n",
    "def genOLSYhats(oos_df, model, lhs_col, rhs_cols):    \n",
    "    # Obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    oos_lhs   = oos_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # add constant\n",
    "    oos_rhs   = np.concatenate((np.ones([len(oos_lhs),1]), oos_rhs),\n",
    "                                axis=1).reshape(-1)\n",
    "    \n",
    "    # Form the yhats\n",
    "    yhats = np.dot(model, oos_rhs)\n",
    "    \n",
    "    # Return results\n",
    "    return pd.DataFrame(data={'date': oos_df.index.values,\n",
    "                              'y': np.array(oos_df['y'].values),\n",
    "                              'yhats': yhats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e614cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLasso(train_df, hps_yhats_dict, lhs_col, rhs_cols):\n",
    "    # Extract hps\n",
    "    alpha = hps_yhats_dict['alpha']\n",
    "\n",
    "    # Obtain the training input and output data and, if passed, validation data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "\n",
    "    # standardize\n",
    "    scaler = StandardScaler()\n",
    "    train_rhs = scaler.fit_transform(train_rhs)\n",
    "\n",
    "    # fit\n",
    "    model = linear_model.Lasso(alpha=alpha, \n",
    "                               selection='cyclic')\n",
    "    model.fit(train_rhs, train_lhs)\n",
    "\n",
    "    # gather what coefs were used\n",
    "    used_coefs = (model.coef_!=0)*1\n",
    "\n",
    "    return model, used_coefs, scaler\n",
    "\n",
    "def genLassoYhats(oos_df, model, lhs_col, rhs_cols, scaler):    \n",
    "    # Obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    oos_lhs   = oos_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # Standarize\n",
    "    oos_rhs   = scaler.transform(oos_rhs)\n",
    "    \n",
    "    # Form the yhats\n",
    "    yhats = model.predict(oos_rhs)\n",
    "    \n",
    "    # Return results\n",
    "    return pd.DataFrame(data={'date': oos_df.index.values,\n",
    "                              'y': np.array(oos_df['y'].values),\n",
    "                              'yhats': yhats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df, last_train_year=2018, val_end_year=2021, arch_name=None, num_cpus=10):\n",
    "    #Initialize hp result objects\n",
    "    results_list    = []\n",
    "    hps_mse_df_list = []\n",
    "\n",
    "    # determine columns\n",
    "    all_cols = list(df.columns.values)\n",
    "    lhs_col  = ['y']\n",
    "    all_cols.remove('y')\n",
    "    rhs_cols = all_cols\n",
    "    assert(df.shape[1]==(len(lhs_col)+len(rhs_cols))) # lhs col + rhs cols\n",
    "\n",
    "    # Initialize the hyperparameter grid for GBDT\n",
    "    learning_rates  = [0.1, 0.05, 0.01] # 0.1, 0.05, 0.01 # [5e-2, 5e-3, 5e-4, 5e-5]\n",
    "    num_ests        = [250] # [100, 250, 500, 1000] \n",
    "    subsamples      = [0.99]  # [0.95, 0.99, 0.999]\n",
    "    min_samp_splits = [0.005] # [0.001, 0.005, 0.01]\n",
    "    max_depths      = [2, 3, 4] # [2, 3, 4, 5]\n",
    "    max_features    = [0.1, 0.15, 0.2] # [0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25]\n",
    "\n",
    "    # Initialize the hyperparameter grid for Lasso\n",
    "    alphas          = [5e-3] # [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n",
    "\n",
    "    # Initialize hyperparameter grid for OLS\n",
    "    window_sizes    = [1000, 5000, 10000, 20000, 35000, 50000, 65000]\n",
    "\n",
    "    # Determine the dates in the validation window  \n",
    "    val_dates = np.unique(df[(df.index.year > last_train_year)  \n",
    "                             & (df.index.year <= val_end_year)].index.values) \n",
    "\n",
    "\n",
    "    # Generate yhats for every hyperparameter grid point\n",
    "    #     for hps in itertools.product(learning_rates,\n",
    "    #                                  num_ests,\n",
    "    #                                  subsamples,\n",
    "    #                                  min_samp_splits,\n",
    "    #                                  max_depths, \n",
    "    #                                  max_features):\n",
    "    #         # build the hp and yhats dictionary\n",
    "    #         hps_yhats_dict = {'learning_rate': hps[0],\n",
    "    #                           'num_est': hps[1],\n",
    "    #                           'subsample': hps[2],\n",
    "    #                           'min_samp_split': hps[3],\n",
    "    #                           'max_depth': hps[4],\n",
    "    #                           'max_feature': hps[5],\n",
    "    #                           'results_df': pd.DataFrame()}\n",
    "    for hps in itertools.product(window_sizes):\n",
    "        hps_yhats_dict = {'window_size': hps[0],\n",
    "                          'results_df': pd.DataFrame()}\n",
    "\n",
    "        # fit on all val dates\n",
    "        print(hps_yhats_dict, '\\n')\n",
    "        tic = time.perf_counter()\n",
    "        def loopOverValDates(val_date):\n",
    "            # Form train and validation data frames\n",
    "            window_size = hps_yhats_dict['window_size']\n",
    "            temp_df  = df[lhs_col+rhs_cols][df.index <= val_date]\n",
    "            train_df = temp_df[temp_df.index < val_date][-window_size:]\n",
    "            val_df   = temp_df[temp_df.index == val_date]\n",
    "\n",
    "            # fit model and generate yhats for val week\n",
    "            model          = fitOLS(train_df, lhs_col, rhs_cols)\n",
    "            val_results_df = genOLSYhats(val_df, model, lhs_col, rhs_cols)\n",
    "\n",
    "            # Return this date's results\n",
    "            return val_results_df, model\n",
    "\n",
    "        # Fit on all the val dates\n",
    "        val_results = Parallel(n_jobs=num_cpus)(delayed(loopOverValDates)(val_date) for val_date in tqdm(val_dates))\n",
    "\n",
    "        # Extract val results\n",
    "        val_results_df = pd.DataFrame()\n",
    "        beta_hats       = np.zeros((len(rhs_cols)+1))\n",
    "        for j in range(len(val_results)):\n",
    "            val_results_df = pd.concat((val_results_df, val_results[j][0]))\n",
    "            beta_hats      += val_results[j][1]\n",
    "        hps_yhats_dict['results_df'] = val_results_df\n",
    "        hps_yhats_dict['beta_hats']  = beta_hats/len(val_dates)\n",
    "\n",
    "\n",
    "\n",
    "#         def loopOverValDates(val_date):\n",
    "#             # Form train and validation data frames\n",
    "#             temp_df  = df[lhs_col+rhs_cols][df.index <= val_date]\n",
    "#             train_df = temp_df[temp_df.index < val_date]\n",
    "#             val_df   = temp_df[temp_df.index == val_date]\n",
    "\n",
    "#             # Fit model and generate yhats for val week\n",
    "#             model          = fitGBDT(train_df, hps_yhats_dict, lhs_col, rhs_cols)\n",
    "#             val_results_df = genYhats(val_df, model, lhs_col, rhs_cols)\n",
    "\n",
    "#             # Obtain training data accuracy for studying over/underfit\n",
    "#             train_acc = model.score(train_df[rhs_cols].values, train_df['y'].values)\n",
    "\n",
    "#             # Return this date's results\n",
    "#             return val_results_df, train_acc\n",
    "\n",
    "#         # Fit on all the val dates\n",
    "#         val_results = Parallel(n_jobs=num_cpus)(delayed(loopOverValDates)(val_date) for val_date in tqdm(val_dates))\n",
    "\n",
    "#         # Extract val results\n",
    "#         val_results_df = pd.DataFrame()\n",
    "#         train_accs     = []\n",
    "#         for j in range(len(val_results)):\n",
    "#             val_results_df = pd.concat((val_results_df, val_results[j][0]))\n",
    "#             train_accs.append(val_results[j][1])\n",
    "#         hps_yhats_dict['results_df'] = val_results_df\n",
    "#         hps_yhats_dict['avg_train_acc'] = np.mean(train_accs)\n",
    "\n",
    "        # Save run time and space out result print out\n",
    "        toc = time.perf_counter()\n",
    "        print('\\n\\n\\n')\n",
    "\n",
    "        # Update this HP point's results and append to list of results\n",
    "        # ys    = hps_yhats_dict['results_df'].y.values\n",
    "        # yhats = (hps_yhats_dict['results_df'].yhats.values>0)*1\n",
    "        # yhats[yhats==0] = -1\n",
    "        # oos_acc = np.sum(ys==yhats)/len(yhats)\n",
    "        # hps_yhats_dict['oos_acc'] = oos_acc\n",
    "        results_list.append(hps_yhats_dict)\n",
    "\n",
    "        # Save the ongoing results as a csv to be able to watch\n",
    "        cv_results_dict = hps_yhats_dict.copy()\n",
    "        del cv_results_dict['results_df']\n",
    "        if 'models' in cv_results_dict:\n",
    "            del cv_results_dict['models']\n",
    "        cv_results_dict['runtime_mins']     = round((toc - tic)/60, 0)  \n",
    "        cv_results_dict['arch_name']        = arch_name\n",
    "        cv_results_dict['first_val_year']   = str(last_train_year+1) \n",
    "        cv_results_dict['first_train_year'] = str(np.min(df.index.year))\n",
    "\n",
    "        #temp_df = hps_yhats_dict['results_df'].copy()\n",
    "        # ys      = temp_df[temp_df.date.dt.year==2021][lhs_col].values.reshape(-1)\n",
    "        # yhats   = temp_df[temp_df.date.dt.year==2021].yhats.values\n",
    "        # yhats   = (yhats>0)*1\n",
    "        # yhats[yhats==0] = -1\n",
    "        # cv_results_dict['oos_acc_2021'] = np.sum(ys==yhats)/len(yhats)\n",
    "\n",
    "#         hps_mse_df_list.append(pd.DataFrame(cv_results_dict, index=[0]))\n",
    "#         cv_df = pd.concat(hps_mse_df_list, ignore_index=True)\n",
    "\n",
    "#         timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         fp = '../4-output/cv-results-' + arch_name +'-' + timestr + '.csv'\n",
    "#         cv_df.to_csv(fp, index=False)\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4558459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelPortfolioWeights(df):\n",
    "    # set parameters for tertile weights\n",
    "    # note: can't go to zero on any as there may be week where that is only yhat\n",
    "    bottom_tertile = 1/6\n",
    "    mid_tertile = 1/3\n",
    "    top_tertile = (1-bottom_tertile-mid_tertile)\n",
    "\n",
    "    # create portfolio weights\n",
    "    df = df.sort_values(by=['date', 'yhat'])\n",
    "    df['counts']  = 1\n",
    "    df['total_assets_per_week_tertile'] = df.groupby(['date', 'yhat']).counts.transform('sum')\n",
    "    df.loc[df.yhat == -1, 'prtfl_wght'] = bottom_tertile / df[df.yhat == -1].total_assets_per_week_tertile\n",
    "    df.loc[df.yhat == 0,  'prtfl_wght'] = mid_tertile / df[df.yhat == 0].total_assets_per_week_tertile\n",
    "    df.loc[df.yhat == 1,  'prtfl_wght'] = top_tertile / df[df.yhat == 1].total_assets_per_week_tertile\n",
    "\n",
    "    # clean up\n",
    "    df = df.drop(['counts', 'total_assets_per_week_tertile'], axis=1)\n",
    "\n",
    "    # fix weeks where portfolio weights add up to less than 1\n",
    "    temp_df = df.groupby(['date'])[['prtfl_wght']].sum()\n",
    "    temp_df = temp_df[~np.isclose(temp_df.prtfl_wght, 1)]\n",
    "    if 0<temp_df.shape[0]:\n",
    "        for i in range(temp_df.shape[0]):\n",
    "            date = temp_df.index.values[i]\n",
    "            total_weight = temp_df.prtfl_wght.values[i]\n",
    "            df.loc[df.index == date, 'prtfl_wght'] = df[df.index==date].prtfl_wght * (1/total_weight)\n",
    "            \n",
    "    # confirm portfolio weights roughly sum to 1 for each week\n",
    "    assert(len(np.unique(df.index)) == \n",
    "           np.sum(np.isclose(df.groupby(['date']).prtfl_wght.sum(), 1,\n",
    "                             rtol=1e-2, atol=1e-2)))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAnnualTransactionCosts(df):\n",
    "    # merge on the previous week's holdings to the new holdings\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df[temp_df.index<np.max(temp_df.index)]\n",
    "    temp_df.index = temp_df.index+np.timedelta64(1, 'D')\n",
    "    temp_df = temp_df[['asset', 'prtfl_wght']]\n",
    "    temp_df = temp_df.rename(columns={'prtfl_wght': 'prtfl_wght_tm7'})\n",
    "    df      = df.merge(temp_df,\n",
    "                       on=['date', 'asset'],\n",
    "                       how='outer',\n",
    "                       validate='one_to_one')\n",
    "\n",
    "    # calc weekly turnover and ensure it has the appropriate range\n",
    "    df['asset_to'] = np.abs(df.prtfl_wght - df.prtfl_wght_tm7)\n",
    "    to_df = df.groupby('date')[['asset_to']].sum().reset_index()\n",
    "    assert((np.min(to_df.asset_to)>=0) & (np.max(to_df.asset_to<=2)))\n",
    "\n",
    "    # correct the first and last week valid for buying the initial port and liquidating\n",
    "    to_df.loc[0, 'asset_to'] = 1\n",
    "    to_df = pd.concat((to_df, pd.DataFrame(data={'date': np.max(temp_df.index.values)+np.timedelta64(1, 'D'),\n",
    "                                                 'asset_to': 1}, index=[0])))\n",
    "    to_df = to_df.reset_index(drop=True)\n",
    "\n",
    "    # add transaction costs assuming maker and taker fee of 20 bps each\n",
    "    to_df['tc'] = to_df.asset_to*0.002\n",
    "\n",
    "    # return annualize transaction cost\n",
    "    return -np.sum(to_df.tc)/(to_df.shape[0]/52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioReturn(r_df):\n",
    "    num_days = r_df.shape[0]\n",
    "    if np.sum(r_df.r.values <= -1)>=1:\n",
    "        return -1\n",
    "    else:\n",
    "        tot_ret   = np.product(r_df.r.values+1)-1\n",
    "        daily_ret = (tot_ret+1)**(1/num_days)-1\n",
    "        annl_ret  = (daily_ret+1)**(365.25)-1\n",
    "        return annl_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa790826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioSharpe(r_df):\n",
    "    daily_sharpe = np.mean(r_df.r.values)/np.std(r_df.r.values)\n",
    "    annl_sharpe  = daily_sharpe*np.sqrt(365.25)\n",
    "    return annl_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983905fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_draw_down(r_df):\n",
    "    cumulative_ret=(r_df.r+1).cumprod()\n",
    "    roll_max=cumulative_ret.rolling(len(cumulative_ret), min_periods=1).max()\n",
    "    daily_drawdown=cumulative_ret/roll_max\n",
    "    max_daily_drawdown=daily_drawdown.min() - 1\n",
    "    return max_daily_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a85a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_1_week_loss(r_df):\n",
    "    max_loss=(r_df['r']+1).rolling(7).apply(np.prod)\n",
    "    max_loss_minus=max_loss.min()-1\n",
    "    return max_loss_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTestYhats(df, opt_hps, test_year=2022): \n",
    "    test_dates = np.unique(df[df.index.year == test_year].index.values)\n",
    "\n",
    "    all_cols = list(df.columns.values)\n",
    "    lhs_col  = ['y']\n",
    "    all_cols.remove('y')\n",
    "    rhs_cols = all_cols\n",
    "\n",
    "    def loopOverOOSDates(test_date):\n",
    "        # build data\n",
    "        temp_df       = df[df.index <= test_date].copy()\n",
    "        train_df      = temp_df[temp_df.index < test_date]\n",
    "        oos_df        = temp_df[temp_df.index == test_date]\n",
    "\n",
    "        # fit and predict\n",
    "        model          = fitGBDT(train_df, opt_hps, lhs_col, rhs_cols)\n",
    "        oos_results_df = genYhats(oos_df, model, lhs_col, rhs_cols)\n",
    "\n",
    "        return oos_results_df\n",
    "\n",
    "    # run\n",
    "    oos_results = Parallel(n_jobs=num_cpus)(delayed(loopOverOOSDates)(test_date) for test_date in tqdm(test_dates))\n",
    "\n",
    "    # Extract oos results\n",
    "    oos_results_df = pd.concat(oos_results)\n",
    "\n",
    "    return oos_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE CODE\n",
    "\n",
    "# Load in the data\n",
    "input_fp = '../3-data/clean/panel_btceth_30min.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "\n",
    "# Clean up the data \n",
    "df = df.set_index('date')\n",
    "df = df.astype('float32')\n",
    "\n",
    "# Select useful variables accoridng to Lasso\n",
    "df = df[['y',\n",
    "         'covar_btc_volume_sum_tm288',\n",
    "         'covar_btc_r_tm36',\n",
    "         'covar_btc_ema_144_t',\n",
    "         'covar_btc_ema_288_t',\n",
    "         'covar_eth_r_tm12',\n",
    "         'covar_eth_ma_24_t',\n",
    "         'covar_btc_rsi_tm288',\n",
    "         'covar_eth_rsi_tm2016',\n",
    "         'covar_eth_rsi_tm4032',\n",
    "         'macro_mcap_ret_ath_t',\n",
    "         'macro_mcap_altcoin_ret_ath_t']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV\n",
    "results_list = runCV(df, last_train_year=2019, val_end_year=2021,\n",
    "                     arch_name='lasso_longshort', num_cpus=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # step 1\n",
    "\n",
    "    # step 2\n",
    "\n",
    "    # step 3\n",
    "\n",
    "    # do gbdt with the full set where i randomly sample sqrt of them and then try with the 36 cols from lasso where i sample half or something\n",
    "    \n",
    "    # TODO explore if normalization helps or not and make sure no forward looking bias\n",
    "# played with the lasso code below to select a final set of 36 features\n",
    "    lasso_feats =  ['covar_btc_covar_trades_t_ma_tm8640',\n",
    "                    'covar_btc_covar_trades_t_min_tm30',\n",
    "                    'covar_btc_covar_trades_t_min_tm60',\n",
    "                    'covar_btc_covar_volume_t_min_tm4320',\n",
    "                    'covar_btc_p_log_t',\n",
    "                    'covar_btc_r_1min_ema_tm360',\n",
    "                    'covar_btc_r_5min_skew_tm720',\n",
    "                    'covar_btc_r_tm1440',\n",
    "                    'covar_btc_r_tm15',\n",
    "                    'covar_btc_r_tm86400',\n",
    "                    'covar_btc_rsi_tm720',\n",
    "                    'covar_eth_covar_trades_t_max_tm4320',\n",
    "                    'covar_eth_covar_trades_t_min_tm20',\n",
    "                    'covar_eth_covar_trades_t_vol_tm60',\n",
    "                    'covar_eth_covar_volume_t_ma_tm60',\n",
    "                    'covar_eth_covar_volume_t_ma_tm8640',\n",
    "                    'covar_eth_covar_volume_t_max_tm4320',\n",
    "                    'covar_eth_covar_volume_t_max_tm720',\n",
    "                    'covar_eth_covar_volume_t_sum_tm20',\n",
    "                    'covar_eth_covar_volume_t_sum_tm60',\n",
    "                    'covar_eth_covar_volume_t_vol_tm360',\n",
    "                    'covar_eth_p_t',\n",
    "                    'covar_eth_r_1min_ema_tm60',\n",
    "                    'covar_eth_r_1min_ma_tm360',\n",
    "                    'covar_eth_r_1min_vol_tm20',\n",
    "                    'covar_eth_r_1min_vol_tm5',\n",
    "                    'covar_eth_r_5min_ma_tm20160',\n",
    "                    'covar_eth_r_5min_ma_tm60',\n",
    "                    'covar_eth_r_5min_min_tm30',\n",
    "                    'covar_eth_r_5min_min_tm720',\n",
    "                    'covar_eth_r_cummax_t',\n",
    "                    'covar_eth_r_cummin_t',\n",
    "                    'covar_eth_r_tm120',\n",
    "                    'covar_eth_r_tm1440',\n",
    "                    'covar_eth_r_tm40320',\n",
    "                    'covar_eth_volume_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_master = results_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ca09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE RESULTS\n",
    "# with open('../3-data/derived/validation_2021_results_btceth_ols.pickle', 'wb') as handle:\n",
    "#     pickle.dump(results_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa665ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study for signal\n",
    "for i in [6, 5, 4, 3, 2, 1, 0]:\n",
    "    print(i)\n",
    "    df             = pd.read_pickle(input_fp)\n",
    "    val_results_df = results_list[i]['results_df']\n",
    "    val_results_df = val_results_df.drop('y', axis=1)\n",
    "    val_results_df = val_results_df.merge(df,\n",
    "                                          on=['date'],\n",
    "                                          how='inner', \n",
    "                                          validate='one_to_one')\n",
    "    val_df = pd.DataFrame(data={'date': val_results_df.date.values,\n",
    "                                'y': val_results_df.y.values,\n",
    "                                'y_btc_eth_r_tp2_tp7': val_results_df['y_btc_eth_r_tp2_tp7'].values,\n",
    "                                'yhat': val_results_df.yhats.values})\n",
    "\n",
    "    val_df['yhat'] = (val_df.yhat-np.min(val_df.yhat))\n",
    "    val_df['yhat'] = val_df.yhat/np.max(val_df.yhat)\n",
    "    for lev in [2, 3, 4, 5]:\n",
    "        for fee in [1e-4, 2e-4, 5e-4]:\n",
    "            print(lev)\n",
    "            print(fee)\n",
    "            yhat_vals = val_df.yhat.values\n",
    "            for yhat_val in yhat_vals:\n",
    "                val_df.loc[val_df.yhat<=yhat_val, 'pw'] = -1\n",
    "                val_df.loc[val_df.yhat>yhat_val, 'pw'] = 1\n",
    "                assert(0==val_df.pw.isnull().sum())\n",
    "                port_ret = np.prod(lev*(1+val_df.pw*val_df.y_btc_eth_r_tp2_tp7)-fee)**(1/len(val_df))-1\n",
    "                val_df.loc[val_df.yhat==yhat_val, 'ret'] = port_ret\n",
    "\n",
    "            val_df = val_df.drop('pw', axis=1)\n",
    "\n",
    "            val_df = val_df.sort_values(by='yhat')\n",
    "            val_df['yhat'] = np.arange(len(val_df))/len(val_df)\n",
    "            plt.plot(val_df.yhat, val_df.ret)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345dcfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10\n",
    "val_df['yhat_decile'] = pd.qcut(val_df.yhat, q=bins, labels=np.arange(bins))\n",
    "val_df.groupby('yhat_decile')['y_btc_eth_r_tp2_tp7'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study feature importance\n",
    "\n",
    "feat_imprt = np.zeros(df.shape[1]-2)\n",
    "models = results_list[0]['models']\n",
    "for model in models:\n",
    "    feat_imprt += model.feature_importances_\n",
    "feat_imprt = feat_imprt/(len(models)-1)\n",
    "print(feat_imprt)\n",
    "\n",
    "# useful features:\n",
    "print('useful features:')\n",
    "threshold = np.median(feat_imprt)\n",
    "for i in np.where(feat_imprt > threshold)[0]:\n",
    "    print(df.columns[2:][i])\n",
    "print('\\nuseless features:')\n",
    "for i in np.where(feat_imprt <= threshold)[0]:\n",
    "    print(df.columns[2:][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form validation period results\n",
    "val_results_df = results_list[0]['results_df']\n",
    "val_results_df = val_results_df.drop('y', axis=1)\n",
    "val_results_df = val_results_df.merge(df,\n",
    "                                      on=['date'],\n",
    "                                      how='inner', \n",
    "                                      validate='one_to_one')\n",
    "val_df = pd.DataFrame(data={'date': val_results_df.date.values,\n",
    "                            'y': val_results_df.y.values,\n",
    "                            'y_btc_eth_r_tp2_tp7': val_results_df['y_btc_eth_r_tp2_tp7'].values,\n",
    "                            'yhat': val_results_df.yhats.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very nice!\n",
    "val_df['yhat_decile'] = pd.qcut(val_df.yhat, q=10, labels=np.arange(10))\n",
    "val_df.groupby('yhat_decile')['y_btc_eth_r_tp2_tp7'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different trading rules\n",
    "trading_cost = 0.001\n",
    "\n",
    "max_return = 0\n",
    "\n",
    "# loop over leverage\n",
    "for leverage in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for long_threshold in range(10):\n",
    "        for short_threshold in range(10):\n",
    "            # grab yhats\n",
    "            yhats = val_df.yhat.values\n",
    "\n",
    "            # figure out thresholds\n",
    "            neg_yhats = yhats[yhats<0]\n",
    "            pos_yhats = yhats[yhats>=0]\n",
    "            neg_threshold = pd.qcut(neg_yhats, q=10).categories[short_threshold].right\n",
    "            pos_threshold = pd.qcut(pos_yhats, q=10).categories[long_threshold].left\n",
    "\n",
    "            # turn yhats into actual long or short position\n",
    "            pos = np.piecewise(yhats, [yhats<=neg_threshold, \n",
    "                                       (yhats>neg_threshold)&(yhats<=pos_threshold), \n",
    "                                       yhats>pos_threshold],\n",
    "                               [-1, 0, 1])\n",
    "\n",
    "            # figure out places to charge me\n",
    "            # by finding indices where previous value is different and the index value is not 0\n",
    "            trading_fee_array = np.concatenate((np.array([trading_cost]), \n",
    "                                               ((pos[1:]!=pos[:-1])&(pos[1:]!=0))*1*trading_cost))\n",
    "\n",
    "            # calc returns after cost\n",
    "            returns = pos*val_df.y_btc_eth_r_tp2_tp7*leverage\n",
    "            returns = returns - trading_fee_array\n",
    "\n",
    "            # report\n",
    "            overall_return = np.prod(returns+1)-1\n",
    "            if overall_return > max_return:\n",
    "                max_return = overall_return\n",
    "                \n",
    "                print('leverage '+str(leverage))\n",
    "                print('long thres '+str(long_threshold))\n",
    "                print('short threshold '+str(short_threshold))\n",
    "                \n",
    "                print('overal return '+str(np.round(overall_return, 4)))\n",
    "                start_index = 0\n",
    "                for i in range(int(2*24*30.5), 10272, int(2*24*30.5)):\n",
    "                    end_index = i\n",
    "                    month_returns = returns[start_index:end_index]\n",
    "                    month_returns = np.prod(month_returns+1)-1\n",
    "\n",
    "                    start_index = i\n",
    "                    print('months return ' +str(np.round(month_returns, 4)))\n",
    "\n",
    "                # report return adjusted by std of negative returns\n",
    "                std_neg_returns = np.std(returns[returns<0])\n",
    "                print('adjusted sharpe '+str(np.round(overall_return/std_neg_returns,4)))\n",
    "\n",
    "                # max dd\n",
    "                cumulative_ret=(returns+1).cumprod()\n",
    "                roll_max=cumulative_ret.rolling(len(cumulative_ret), min_periods=1).max()\n",
    "                dd=cumulative_ret/roll_max\n",
    "                max_dd=dd.min() - 1\n",
    "                print('max dd '+str(np.round(max_dd, 4)))\n",
    "\n",
    "                # sharpe\n",
    "                sharpe = np.mean(returns)/np.std(returns)*np.sqrt(365.25*24*2)\n",
    "                print('sharpe '+str(np.round(sharpe, 4)))\n",
    "                print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d43202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO THERE IS MORE TO GRAB FROM v-benchmark_gbdt_btceth_ls.ipynb as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in [2016, 2017, 2018, 2019, 2020, 2021]:\n",
    "#     lasso_df = df[df.date.dt.year == year].copy()\n",
    "\n",
    "#     # set lasso parameters\n",
    "#     rhs_feats = list(included_feats)\n",
    "#     lhs_col   = 'y_btc_eth_diff_r_tp5_tp370'\n",
    "#     alpha     = 0.001\n",
    "\n",
    "#     # scale RHS\n",
    "#     y = lasso_df[lhs_col].values\n",
    "#     X = lasso_df[rhs_feats].values\n",
    "#     X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "#     # fit\n",
    "#     model = Lasso(alpha=alpha, fit_intercept=False)\n",
    "#     model.fit(X_scaled, y)\n",
    "\n",
    "#     # show selected feats\n",
    "#     lasso_feats = [rhs_feats[i] for i in np.nonzero(model.coef_)[0]]\n",
    "#     print(year)\n",
    "#     print(lasso_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14903da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO MOVE ANY FUNCTIONS THAT I MAY USE ELSEWHERE TO COMMON FOLDER SHARED ACROSS PROJECTS THAT I JUST IMPORT WHEN NEEDED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
