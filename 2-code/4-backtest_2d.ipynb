{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e1e7ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0828b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGeomAvg(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the geometric average of a vector of simple returns.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar geometric average.\n",
    "    \"\"\"\n",
    "    if not isinstance(returns, np.ndarray):\n",
    "        raise TypeError(\"Input 'returns' must be a NumPy array\")\n",
    "    if annualized and periods_in_year is None:\n",
    "        raise ValueError(\"Input 'periods_in_year' must be provided if 'annualized' is True\")\n",
    "    geom_avg_at_given_freq = np.prod(1 + returns) ** (1 / np.size(returns)) - 1\n",
    "    return (geom_avg_at_given_freq + 1) ** periods_in_year - 1 if annualized else geom_avg_at_given_freq\n",
    "\n",
    "def calcTSAvgReturn(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float:\n",
    "    \"\"\" Calculate the time series mean return of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar time series mean return.\n",
    "    \"\"\"\n",
    "    mean_ret_at_given_freq = np.mean(returns)\n",
    "    if annualized == False:\n",
    "        return mean_ret_at_given_freq\n",
    "    else:\n",
    "        mean_ret = periods_in_year*mean_ret_at_given_freq\n",
    "        if mean_ret < -1:\n",
    "            return -1.\n",
    "        else:\n",
    "            return mean_ret\n",
    "\n",
    "def calcTotalReturn(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float:\n",
    "    \"\"\" Calculate the total return of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar total return.\n",
    "    \"\"\"\n",
    "    total_return = np.prod(1+returns)-1\n",
    "    if annualized==False:\n",
    "        return total_return\n",
    "    else:\n",
    "        return (total_return+1)**(periods_in_year/len(returns))-1\n",
    "\n",
    "def calcSD(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the standard deviation of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar standard deviation.\n",
    "    \"\"\"\n",
    "    sd_at_given_freq = np.std(returns)\n",
    "    if annualized==False:\n",
    "        return sd_at_given_freq\n",
    "    else:\n",
    "        return np.sqrt(periods_in_year)*sd_at_given_freq\n",
    "\n",
    "def calcSharpe(returns: np.array,\n",
    "    periods_in_year: int,\n",
    "    risk_free_returns: np.array=None) -> float:\n",
    "    \"\"\" Calculate the annual Sharpe Ratio of a vector of simple returns. \n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "        risk_free_returns (np.array): vector of simple returns of the risk free rate.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar standard deviation.\n",
    "    \"\"\"\n",
    "    if risk_free_returns is not None:\n",
    "        returns = returns - risk_free_returns\n",
    "    \n",
    "    return (calcTSAvgReturn(returns, annualized=True, periods_in_year=periods_in_year) /\n",
    "            calcSD(returns, annualized=True, periods_in_year=periods_in_year))\n",
    "\n",
    "def calcMaxDrawdown(returns: np.array) -> float:\n",
    "    ''' calculate maximum drawdown for a vector of returns of any frequency.\n",
    "    \n",
    "    Args:\n",
    "        returns (np.array): vector of simple returns.\n",
    "    \n",
    "    Returns:\n",
    "        max_drawdown (float): maximum drawdown in simple return units over this period.\n",
    "    '''\n",
    "    # calculate the cumulative return as a new vector of the same length\n",
    "    cumulative_ret=(returns+1).cumprod()\n",
    "\n",
    "    # for every period, calc the historic maximum value of the portfolio \n",
    "    roll_max=pd.Series(cumulative_ret).rolling(len(cumulative_ret), min_periods=1).max()\n",
    "\n",
    "    # calc drawdown as the current portfolio value divided by the historic max value\n",
    "    dd=np.min(cumulative_ret/roll_max)\n",
    "    \n",
    "    # return simple return of max drawdown\n",
    "    return dd-1\n",
    "\n",
    "def calcMaxOneWeekLoss(returns: np.array, periods_in_week: int) -> float:\n",
    "    ''' Calculate the maximum loss for a one week period given how many obs are in week for input\n",
    "        returns.\n",
    "    \n",
    "    Args:\n",
    "        returns (np.array): vector of simple returns of any frequency.\n",
    "        periods_in_week (int): number of observations in a week.\n",
    "    \n",
    "    Returns:\n",
    "        max_loss (float): maximum loss over any one week period in simple returns.\n",
    "    '''\n",
    "    weekly_returns = (pd.Series(returns)+1).rolling(periods_in_week).apply(np.prod)\n",
    "    max_loss = weekly_returns.min()-1\n",
    "    return max_loss\n",
    "\n",
    "def calcTransactionCosts(positions: np.array) -> np.array:\n",
    "    ''' Calculate a vector of transaction costs which are positive numbers in return units.\n",
    "    \n",
    "    Args:\n",
    "        positions (np.array): vector of positions, where positive is long and above 1, in absolute\n",
    "                              value terms, is a leveraged position.\n",
    "\n",
    "    Returns:\n",
    "        tc (np.array): vector of transaction costs in return terms.\n",
    "    '''\n",
    "    # transaction costs, in return terms, from kraken for trading two spots paris, on margin\n",
    "    tc_to_open        = 0.0010\n",
    "    tc_to_close       = 0.0010\n",
    "    tc_to_open_margin = 0.00015\n",
    "    tc_margin_2_d     = 0.0024\n",
    "\n",
    "    # initial tc array\n",
    "    tc = np.zeros(len(positions))\n",
    "\n",
    "    # set first tc\n",
    "    first_position = positions[0]\n",
    "    if first_position == 0:\n",
    "        tc[0] = 0\n",
    "    elif (-1 <= first_position) & (first_position <= 1):\n",
    "        tc[0] = tc_to_open\n",
    "    elif (-5 <= first_position) & (first_position <= 5):\n",
    "        tc[0] = tc_to_open+tc_to_open_margin+tc_margin_2_d\n",
    "    else:\n",
    "        raise ValueError('first position is not a valid position.')\n",
    "\n",
    "    # set remaining tc's\n",
    "    for i in range(1,len(tc)):\n",
    "        prev_position = positions[i-1]\n",
    "        current_position = positions[i]\n",
    "        if current_position == prev_position:\n",
    "            if np.abs(current_position)>1:\n",
    "                tc[i] = tc_margin_2_d\n",
    "        else:\n",
    "            if current_position==0:\n",
    "                tc[i] = tc_to_close\n",
    "            elif (-1 <= current_position) & (current_position <= 1):\n",
    "                tc[i] = tc_to_close+tc_to_open\n",
    "            elif (-5 <= current_position) & (current_position <= 5):\n",
    "                tc[i] = tc_to_close+tc_to_open+tc_to_open_margin+tc_margin_2_d\n",
    "            else: \n",
    "                raise ValueError('position '+str(i)+' is not a valid position.')\n",
    "\n",
    "    # adjust last tc element for closing position\n",
    "    last_position = positions[-1]\n",
    "    if np.abs(last_position)>0:\n",
    "        tc[-1] += tc_to_close\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bf0d8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitRF(train_df: pd.DataFrame,\n",
    "    lhs_col: str,\n",
    "    rhs_cols: list,\n",
    "    hps: dict,\n",
    "    ys_real: np.array) -> RandomForestClassifier: \n",
    "    ''' Fit random forest on given training data using RHS to predict LHS with given hps.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): training data.\n",
    "        lhs_col (str): LHS column to predict.\n",
    "        rhs_cols (list): list of strings of RHS features.\n",
    "        hps (dict): key value pairs for hyperparameters to set including:\n",
    "                    `num_estimators`, `subsample`, `min_samp_leaf`, `max_depth`, \n",
    "                    `max_feature`, `loss_func`, and `weight_func`.\n",
    "        ys_real (np.array): vector of real valued lhs variable to use to weight.\n",
    "\n",
    "    Returns:\n",
    "        (RandomForestClassifier): fitted model.\n",
    "    '''\n",
    "    # obtain the rhs and lhs data\n",
    "    train_rhs   = train_df[rhs_cols].values.astype('float32')\n",
    "    train_lhs   = train_df[lhs_col].values.reshape(-1).astype('int')\n",
    "    \n",
    "    # build the model\n",
    "    model = RandomForestClassifier(n_estimators=hps['num_estimator'],\n",
    "        criterion=hps['loss_func'],\n",
    "        max_depth=hps['max_depth'],\n",
    "        min_samples_leaf=hps['min_samp_leaf'],\n",
    "        max_features=hps['max_feature'],\n",
    "        n_jobs=4,\n",
    "        random_state=int(hps['num_estimator']\n",
    "            *hps['subsample']\n",
    "            *hps['min_samp_leaf']\n",
    "            *hps['max_depth']\n",
    "            *hps['max_feature']),\n",
    "        class_weight=hps['class_balance'],\n",
    "        max_samples=hps['subsample'])\n",
    "\n",
    "    # calculate sample weights as linearly spaced from 0 to max value s.t. it sums to one\n",
    "    num_samples = train_lhs.shape[0]\n",
    "    if hps['weight_func'] == 'linear':\n",
    "        weights = np.arange(0, num_samples)\n",
    "        weights = weights/np.sum(weights)\n",
    "        epsilon = weights[1]/2\n",
    "        weights[0] = epsilon\n",
    "        weights[-1] -= epsilon\n",
    "    elif hps['weight_func'] == 'uniform':\n",
    "        weights = np.ones(num_samples)/num_samples\n",
    "    elif hps['weight_func'] == 'log':\n",
    "        weights = np.log(np.arange(1,num_samples+1))\n",
    "        weights = weights / np.sum(weights)\n",
    "    elif hps['weight_func'] == 'exp':\n",
    "        weights = np.arange(num_samples)**1.1\n",
    "        weights = weights / np.sum(weights)\n",
    "    elif hps['weight_func'] == 'y':\n",
    "        weights = np.abs(ys_real) / np.sum(np.abs(ys_real))\n",
    "    elif hps['weight_func'] == None:\n",
    "        weights = hps['weight_func']\n",
    "    else:\n",
    "        raise ValueError('the weights function must be set to linear, uniform, log, or exp.')\n",
    "\n",
    "    # fit\n",
    "    model.fit(X=train_rhs, \n",
    "        y=train_lhs, \n",
    "        sample_weight=weights)\n",
    "\n",
    "    # obtain training yhats and accuracy\n",
    "    train_yhats = model.predict(train_rhs)\n",
    "    train_acc = model.score(train_rhs, train_lhs)\n",
    "\n",
    "    return model, train_acc, train_yhats\n",
    "\n",
    "def genRFYhats(oos_df: pd.DataFrame, \n",
    "    model: RandomForestClassifier, \n",
    "    lhs_col: str, \n",
    "    rhs_cols: list) -> int: \n",
    "    \"\"\" Generate predicted probabilities for input data using a random forest classifier model.\n",
    "\n",
    "    Args:\n",
    "        oos_df (pd.DataFrame): out of sample data to fit on.\n",
    "        model (RandomForestClassifier): trained random forest model.\n",
    "        lhs_col (str): name of the column containing the label data.\n",
    "        rhs_cols (list): column names containing the feature data.\n",
    "\n",
    "    Returns:\n",
    "        (int): predicted scalar class.\n",
    "    \"\"\"\n",
    "    # obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values.astype('float32')\n",
    "    \n",
    "    # form the yhats\n",
    "    yhat = model.predict(oos_rhs)\n",
    "    \n",
    "    # Return results\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3471d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df: pd.DataFrame, \n",
    "    lhs_col: str,\n",
    "    val_start_date: str,\n",
    "    num_cpus: int,\n",
    "    arch_name: str, \n",
    "    out_csv_fp: str) -> list:\n",
    "    ''' run step-forward cross validation to select optimal hyperparameters for target model \n",
    "        returning fitting yhats and models as well as outputting results to csv.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): panel of training and val data with date index, LHS variable named \n",
    "                           `lhs_col`, and remaining cols are RHS features.\n",
    "        lhs_col (str): name of the lhs target column.\n",
    "        val_start_date (str): the first date for the validation period.\n",
    "        val_end_date (str): the last date for the validation period.\n",
    "        num_cpus (int): number of cpus to use when parallelizing.\n",
    "        arch_name (str): name of architecture to use when saving intermitent results.\n",
    "        out_csv_fp (str): filepath to output the csv file without `.csv' on end.\n",
    "          \n",
    "    Returns:\n",
    "        (list): list of dictionaries for each hyperparameter fit where each list contains keys of:\n",
    "                    `hps` is dict of hyperparameter combination,\n",
    "                    `yhats` is an array of validation period yhats, and,\n",
    "                    `models` is list of fitted models.  \n",
    "    '''\n",
    "    # initialize args\n",
    "    val_dates = np.unique(df[val_start_date:].index.values)\n",
    "    results_list = []\n",
    "    csv_dict_list = []\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # initialize hp grid for gbdt\n",
    "    num_estimators  = [30]\n",
    "    subsamples      = [0.99]\n",
    "    min_samp_leafs  = [2]\n",
    "    max_depths      = [3]\n",
    "    max_features    = [0.65]\n",
    "    loss_funcs      = ['log_loss']\n",
    "    weight_funcs    = ['y']\n",
    "    class_balances  = ['balanced']\n",
    "    ret_thresholds  = [0]\n",
    "\n",
    "    # loop over all hp combos\n",
    "    for hps in itertools.product(num_estimators,\n",
    "                                 subsamples,\n",
    "                                 min_samp_leafs,\n",
    "                                 max_depths, \n",
    "                                 max_features,\n",
    "                                 loss_funcs,\n",
    "                                 weight_funcs,\n",
    "                                 class_balances,\n",
    "                                 ret_thresholds):\n",
    "        # initialize args\n",
    "        results_dict = {}\n",
    "        results_dict['hps'] = {'num_estimator': hps[0],\n",
    "            'subsample': hps[1],\n",
    "            'min_samp_leaf': hps[2],\n",
    "            'max_depth': hps[3],\n",
    "            'max_feature': hps[4],\n",
    "            'loss_func': hps[5],\n",
    "            'weight_func': hps[6],\n",
    "            'class_balance': hps[7],\n",
    "            'ret_threshold': hps[8]}\n",
    "        print(results_dict['hps'], '\\n') # monitor progress\n",
    "\n",
    "        # form lhs\n",
    "        cv_df = df.copy()\n",
    "        ret_threshold = results_dict['hps']['ret_threshold']\n",
    "        cv_df.loc[cv_df[lhs_col] > ret_threshold, 'y'] = 1\n",
    "        cv_df.loc[cv_df[lhs_col] < -ret_threshold, 'y'] = 0\n",
    "\n",
    "        # save val period returns\n",
    "        btc_eth_diff = cv_df[val_start_date:][lhs_col].values\n",
    "\n",
    "        # fit model on all val dates\n",
    "        tic = time.perf_counter()\n",
    "        def loopOverValDates(val_date): # set up as a func to loop over\n",
    "            # form train and val data\n",
    "            train_df = cv_df[cv_df.index < val_date].copy()\n",
    "            train_y_btc_eth_diff = train_df[lhs_col].values\n",
    "            train_df = train_df.drop(lhs_col, axis=1)\n",
    "            val_df   = cv_df[cv_df.index == val_date].copy()\n",
    "            val_df   = val_df.drop(lhs_col, axis=1)\n",
    "\n",
    "            # fit model and generate yhats for val week\n",
    "            model, train_acc, train_yhats = fitRF(train_df, 'y', rhs_cols, hps=results_dict['hps'],\n",
    "                                                  ys_real = train_y_btc_eth_diff)\n",
    "            yhats = genRFYhats(val_df, model, 'y', rhs_cols)\n",
    "\n",
    "            return yhats, model, train_acc, train_yhats\n",
    "\n",
    "        val_results = Parallel(n_jobs=int(num_cpus/4))(delayed(loopOverValDates)(val_date) for val_date in tqdm(val_dates))\n",
    "\n",
    "        # extract validation periods results\n",
    "        yhats_list = []\n",
    "        models_list = []\n",
    "        train_acc_list = []\n",
    "        train_yhats_list = []\n",
    "        for t in range(len(val_results)):\n",
    "            yhats_list.append(val_results[t][0])\n",
    "            models_list.append(val_results[t][1])\n",
    "            train_acc_list.append(val_results[t][2])\n",
    "            train_yhats_list.append(val_results[t][3])\n",
    "        yhats = np.array(yhats_list)*2-1\n",
    "        ys    = cv_df[val_start_date:].y.values*2-1\n",
    "        results_dict['yhats'] = yhats\n",
    "\n",
    "        # save results to master result list    \n",
    "        results_list.append(results_dict)\n",
    "\n",
    "        # save results to csv to monitor during cv\n",
    "        toc = time.perf_counter()\n",
    "        csv_dict = results_dict['hps'].copy()\n",
    "        del results_dict\n",
    "        csv_dict['arch_name'] = arch_name\n",
    "        csv_dict['val_start_date'] = val_start_date\n",
    "        csv_dict['val_end_date'] = np.datetime_as_string(np.max(df.index.values))[:10]\n",
    "        csv_dict['runtime_mins'] = round((toc - tic)/60, 0) \n",
    "        total_obs = 0\n",
    "        total_yhat_long = 0\n",
    "        total_yhat_short = 0\n",
    "        for train_yhats in train_yhats_list:\n",
    "            train_yhats = train_yhats*2 - 1\n",
    "            total_obs += train_yhats.shape[0]\n",
    "            total_yhat_long += np.sum(train_yhats==1)\n",
    "            total_yhat_short += np.sum(train_yhats==-1)\n",
    "        csv_dict['yhat_long_pct_train'] = total_yhat_long / total_obs\n",
    "        csv_dict['yhat_short_pct_train'] = total_yhat_short / total_obs\n",
    "        csv_dict['accuracy_train'] = np.mean(np.array(train_acc_list))\n",
    "        csv_dict['yhat_long_pct'] = np.sum(yhats==1)/len(yhats)\n",
    "        csv_dict['yhat_short_pct'] = np.sum(yhats==-1)/len(yhats)\n",
    "        csv_dict['accuracy'] = np.sum(yhats == ys)/len(ys)\n",
    "        tc = calcTransactionCosts(yhats)\n",
    "        returns = yhats*btc_eth_diff - tc\n",
    "        csv_dict['geom_mean'] = calcGeomAvg(returns)\n",
    "        csv_dict['sharpe'] = calcSharpe(returns, periods_in_year=int(365/2))\n",
    "        csv_dict['sd_ann'] = calcSD(returns, annualized=True, periods_in_year=int(365/2))\n",
    "        csv_dict['ts_mean_ann'] = calcTSAvgReturn(returns, annualized=True, periods_in_year=int(365/2))\n",
    "        for lev in [2, 3, 4, 5]:\n",
    "            lev_yhats = yhats*lev\n",
    "            lev_tc = calcTransactionCosts(lev_yhats)\n",
    "            lev_returns = lev_yhats*btc_eth_diff - lev_tc \n",
    "            csv_dict['geom_mean_x'+str(lev)] = calcGeomAvg(lev_returns)\n",
    "            csv_dict['sharpe_x'+str(lev)] = calcSharpe(lev_returns, periods_in_year=int(365/2))\n",
    "            csv_dict['sd_ann_x'+str(lev)] = calcSD(lev_returns, annualized=True, periods_in_year=int(365/2))\n",
    "            csv_dict['ts_mean_ann_x'+str(lev)] = calcTSAvgReturn(lev_returns, annualized=True, periods_in_year=int(365/2))\n",
    "        csv_dict_list.append(csv_dict)\n",
    "        results_df = pd.DataFrame(csv_dict_list)\n",
    "\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = out_csv_fp+'_'+arch_name+'_'+timestr+'.csv'\n",
    "        results_df.to_csv(fp, index=False)\n",
    "\n",
    "        # output results to track\n",
    "        print(csv_dict['runtime_mins'])\n",
    "        print(f\"accuracy: {csv_dict['accuracy']:.4f}\")\n",
    "\n",
    "        print('\\n\\n\\n')\n",
    "        gc.collect()\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "349eb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotYvsYhat(y: np.array, yhats: np.array):\n",
    "    ''' plot average values of y against bins of yhat.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): vector of target variable.\n",
    "        yhats (np.array): vector of fitted values.\n",
    "    '''\n",
    "    # form data\n",
    "    temp_df = pd.DataFrame(data={'y': y, 'yhat': yhats})\n",
    "\n",
    "    # plot\n",
    "    temp_df.groupby('yhat')['y'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "52e92547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectOptimalBacktestedModel(cv_results: list,\n",
    "    df: pd.DataFrame, \n",
    "    start_date: str, \n",
    "    end_date: str, \n",
    "    lhs_col: str) -> dict:\n",
    "    ''' Select the optimal model for the cross validation exercise where we test out various\n",
    "        leverage options to select max sharpe such that it has positive geom avg return.\n",
    "\n",
    "    Args:\n",
    "        cv_results (list): list of dictionaries of backtest results.\n",
    "        df (pd.DataFrame): dataframe to use for studying model performance.\n",
    "        start_date (str): start date of the out of sample period.\n",
    "        end_date (str): end date of the out of sample period.\n",
    "        lhs_col (str): return difference for instrument.\n",
    "    \n",
    "    Returns:\n",
    "        opt_model_dict (dict): the optimal model's hyperparameters and associated yhats.    \n",
    "    '''\n",
    "    # initialize objects\n",
    "    oos_df = df[start_date:end_date].copy()\n",
    "    btc_eth_diff = oos_df[lhs_col].values\n",
    "    max_sharpe = 0\n",
    "\n",
    "    # study each model's results\n",
    "    for results_dict in cv_results:\n",
    "        # obtain model's yhats\n",
    "        yhats = results_dict['yhats']\n",
    "\n",
    "        # calc model performance for all leverages\n",
    "        for lev in [0.25, 0.5, 0.75, 1, 2, 3, 4, 5]:\n",
    "            lev_yhats = yhats*lev\n",
    "            lev_tc = calcTransactionCosts(lev_yhats)\n",
    "            lev_returns = lev_yhats*btc_eth_diff - lev_tc \n",
    "            geom_mean = calcGeomAvg(lev_returns)\n",
    "            sharpe = calcSharpe(lev_returns, periods_in_year=int(365/2))\n",
    "\n",
    "            # save optimal model \n",
    "            if (geom_mean > 0) & (sharpe > max_sharpe):\n",
    "                max_sharpe = sharpe\n",
    "                opt_model_dict = results_dict['hps']\n",
    "                opt_model_dict['lev'] = lev\n",
    "                optimal_yhats = yhats\n",
    "\n",
    "    # report optimal model statistics\n",
    "    lev_yhats = opt_model_dict['lev']*optimal_yhats\n",
    "    lev_tc = calcTransactionCosts(lev_yhats)\n",
    "    lev_returns = lev_yhats*btc_eth_diff - lev_tc \n",
    "    geom_mean = calcGeomAvg(lev_returns)\n",
    "    sharpe = calcSharpe(lev_returns, periods_in_year=int(365/2))\n",
    "    print('The optimal model has the following hyperparameters: ')\n",
    "    print(opt_model_dict)\n",
    "    print(f\"Geometric average 2 day return: {geom_mean:.6f}\")\n",
    "    print(f\"Annualized Sharpe: {sharpe:.3f}\")\n",
    "\n",
    "    # report average return by yhat bin\n",
    "    ret_threshold = opt_model_dict['ret_threshold']\n",
    "    temp_df = oos_df[[lhs_col]].copy()\n",
    "    temp_df.loc[temp_df[lhs_col] > ret_threshold, 'y'] = 1\n",
    "    temp_df.loc[temp_df[lhs_col] < -ret_threshold, 'y'] = -1\n",
    "    plotYvsYhat(y=temp_df.y.values, yhats=yhats)\n",
    "\n",
    "    # save yhats and return\n",
    "    opt_model_dict['yhats'] = optimal_yhats\n",
    "    return opt_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6b38eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportBootstrapTstat(yhats: np.array,\n",
    "    y_real: np.array,\n",
    "    possible_positions: list,\n",
    "    bs_size: int=1000):\n",
    "    ''' print out bootstrapped standard error and t stat to see if oos returns are stat sig.\n",
    "\n",
    "    Args:\n",
    "        yhats (np.array): vector of actual portfolio positions each period for my portfolio.\n",
    "        y_real (np.array): vector of real return for instrument.\n",
    "        possible_positions (list): vector of possible portfolio positions for bs ports to take.\n",
    "        bs_size (int): number of bs samples to generate.\n",
    "    '''\n",
    "    # generate BS vectors of length periods selecting from possible_positions\n",
    "    periods = len(y_real)\n",
    "    portfolios_list = []\n",
    "    for i in range(bs_size):\n",
    "        np.random.seed(i)\n",
    "        portfolios_list.append(np.random.choice(possible_positions, size=periods))\n",
    "\n",
    "    # calc geom BS returns\n",
    "    bs_returns_list = []\n",
    "    for i in range(bs_size):\n",
    "        lev_position = portfolios_list[i]\n",
    "        lev_tc = calcTransactionCosts(lev_position)\n",
    "        lev_returns = lev_position*y_real - lev_tc \n",
    "        geom_mean = calcGeomAvg(lev_returns)\n",
    "        bs_returns_list.append(geom_mean)\n",
    "\n",
    "    # calc bs standard error\n",
    "    bs_se = np.std(bs_returns_list)\n",
    "\n",
    "    # report standard errors\n",
    "    my_tc = calcTransactionCosts(yhats)\n",
    "    my_returns = yhats*y_real - my_tc\n",
    "    geom_mean_me = calcGeomAvg(my_returns)\n",
    "    print(f\"my portfolio geom avg return: {geom_mean_me:.6f}\")\n",
    "    print(f\"bootstrapped standard error is {bs_se:.6f}\")\n",
    "    print(f\"t stat is: {geom_mean_me/bs_se:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ae4f99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTestPeriod(df: pd.DataFrame,\n",
    "    lhs_col: str,\n",
    "    test_start_date: str,\n",
    "    test_end_date: str,\n",
    "    opt_model_dict: dict,\n",
    "    num_cpus: int) -> np.array:\n",
    "    ''' predict for test period using backtested model.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): panel of training and test data with date index, LHS variable named \n",
    "                           `lhs_col`, and remaining cols are RHS features.\n",
    "        lhs_col (str): name of the lhs target column.\n",
    "        test_start_date (str): the first date for the test period.\n",
    "        test_end_date (str): the last date for the test period.\n",
    "        opt_model_dict (dict): hyperparameters for optimal backtested model.\n",
    "        num_cpus (int): number of cpus to use when parallelizing.\n",
    "    \n",
    "    Returns:\n",
    "        yhats (np.array): vector of yhat positions for entire test period.\n",
    "    '''\n",
    "    # form list of rhs col\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # form lhs for oos df\n",
    "    oos_df = df.copy()\n",
    "    ret_threshold = opt_model_dict['ret_threshold']\n",
    "    oos_df.loc[oos_df[lhs_col] > ret_threshold, 'y'] = 1\n",
    "    oos_df.loc[oos_df[lhs_col] < -ret_threshold, 'y'] = 0\n",
    "\n",
    "    # form test dates to loop over\n",
    "    test_dates = np.unique(oos_df[test_start_date:test_end_date].index.values)\n",
    "\n",
    "    def loopOverTestDates(test_date):\n",
    "        # form train and val data\n",
    "        temp_df   = oos_df[oos_df.index <= test_date].copy()\n",
    "        train_df  = temp_df[temp_df.index < test_date].copy()\n",
    "        train_y_btc_eth_diff = train_df[lhs_col].values\n",
    "        train_df  = train_df.drop(lhs_col, axis=1)\n",
    "        test_df   = temp_df[temp_df.index == test_date].copy()\n",
    "        test_df   = test_df.drop(lhs_col, axis=1)\n",
    "\n",
    "        # fit and predict\n",
    "        model, train_acc, train_yhats = fitRF(train_df, 'y', rhs_cols, \n",
    "                                            hps=opt_model_dict, \n",
    "                                            ys_real=train_y_btc_eth_diff)\n",
    "        yhats = genRFYhats(test_df, model, 'y', rhs_cols)\n",
    "\n",
    "        return yhats, model, train_acc, train_yhats\n",
    "\n",
    "    # predict for entire test period\n",
    "    test_results = Parallel(n_jobs=int(num_cpus/4))(delayed(loopOverTestDates)(test_date) for test_date in tqdm(test_dates))\n",
    "\n",
    "    # extract test yhats to return\n",
    "    yhats_list = []\n",
    "    for t in range(len(test_results)):\n",
    "        yhats_list.append(test_results[t][0])\n",
    "    yhats = np.array(yhats_list)*2-1\n",
    "\n",
    "    return yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "63c630c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportTestPeriodResults(yhats: np.array, y_real: np.array):\n",
    "    ''' report various portfolio statistics for the test period.\n",
    "    \n",
    "    Args:\n",
    "        yhats (np.array): vector of actual portfolio positions.\n",
    "        y_real (np.array): vector of real return difference for target instrument.\n",
    "    '''\n",
    "    # calculate returns post transaction costs\n",
    "    tc = calcTransactionCosts(yhats)\n",
    "    returns = yhats*y_real - tc\n",
    "    \n",
    "    # calc portfolio statistics\n",
    "    geom_mean = calcGeomAvg(returns)\n",
    "    sharpe    = calcSharpe(returns, periods_in_year=int(365/2))\n",
    "    max_dd    = calcMaxDrawdown(returns)\n",
    "    max_1week = calcMaxOneWeekLoss(returns, periods_in_week=3)\n",
    "\n",
    "    # report\n",
    "    print(f\"Geometric average 2 day return: {geom_mean:.6f}\")\n",
    "    print(f\"Annualized Sharpe: {sharpe:.3f}\")\n",
    "    print(f\"Maximum drawdown: {max_dd:.4f}\")\n",
    "    print(f\"Maximum loss over any one week period: {max_1week:.4f}\")\n",
    "\n",
    "    # plot classification\n",
    "    plotYvsYhat(y=y_real, yhats=yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dbf55b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_estimator': 30, 'subsample': 0.99, 'min_samp_leaf': 2, 'max_depth': 3, 'max_feature': 0.65, 'loss_func': 'log_loss', 'weight_func': 'y', 'class_balance': 'balanced', 'ret_threshold': 0} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:05<00:00, 26.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "accuracy: 0.5109\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The optimal model has the following hyperparameters: \n",
      "{'num_estimator': 30, 'subsample': 0.99, 'min_samp_leaf': 2, 'max_depth': 3, 'max_feature': 0.65, 'loss_func': 'log_loss', 'weight_func': 'y', 'class_balance': 'balanced', 'ret_threshold': 0, 'lev': 1}\n",
      "Geometric average 2 day return: 0.001650\n",
      "Annualized Sharpe: 0.968\n",
      "my portfolio geom avg return: 0.001650\n",
      "bootstrapped standard error is 0.006744\n",
      "t stat is: 0.24\n",
      "\n",
      "\n",
      " Test period results!! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:02<00:00, 33.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric average 2 day return: 0.002286\n",
      "Annualized Sharpe: 1.172\n",
      "Maximum drawdown: -0.1778\n",
      "Maximum loss over any one week period: -0.0952\n",
      "my portfolio geom avg return: 0.002286\n",
      "bootstrapped standard error is 0.009026\n",
      "t stat is: 0.25\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    in_fp           = '../1-data/clean/bars_ethbtc_2d.pkl'\n",
    "    cv_out_fp       = '../3-output/cv_results'\n",
    "    freq            = 2880\n",
    "    lhs_col         = 'y_ethbtc_r_tp5_tp' + str(freq+5)\n",
    "    val_start_date  = '2021-10-01'\n",
    "    val_end_date    = '2022-06-30'\n",
    "    test_start_date = '2022-07-01'\n",
    "    test_end_date   = '2022-12-31'\n",
    "    num_cpus        = 20 # do factor of 4\n",
    "\n",
    "    # read in and prep training+validation data\n",
    "    df = pd.read_pickle(in_fp)\n",
    "    df = df.set_index('date')\n",
    "    df = df.astype('float32')\n",
    "    \n",
    "    # run the cv\n",
    "    cv_results = runCV(df[:val_end_date],\n",
    "        lhs_col=lhs_col,\n",
    "        val_start_date=val_start_date,\n",
    "        num_cpus=num_cpus,\n",
    "        arch_name='rf_binary_2d',\n",
    "        out_csv_fp=cv_out_fp)\n",
    "\n",
    "    # select optimal optimal\n",
    "    opt_model_dict = selectOptimalBacktestedModel(cv_results, df, start_date=val_start_date, end_date=val_end_date, lhs_col=lhs_col)\n",
    "\n",
    "    # confirm stat sig\n",
    "    reportBootstrapTstat(yhats=opt_model_dict['yhats']*opt_model_dict['lev'],\n",
    "        y_real=df[val_start_date:val_end_date][lhs_col].values,\n",
    "        possible_positions=[-5, -4, -3, -2, -1, -0.75, 0.5, 0.25, 0, 0.25, 0.5, 0.75, 1, 2, 3, 4, 5])\n",
    "\n",
    "    # predict for the test period\n",
    "    print('\\n\\n Test period results!! \\n')\n",
    "    del opt_model_dict['yhats']\n",
    "    test_yhats = predictTestPeriod(df, lhs_col, \n",
    "        test_start_date, test_end_date, \n",
    "        opt_model_dict, num_cpus)\n",
    "\n",
    "    # report test period results\n",
    "    test_yhats  = test_yhats*opt_model_dict['lev']\n",
    "    test_y_real = df[test_start_date:test_end_date][lhs_col].values\n",
    "    reportTestPeriodResults(yhats=test_yhats,\n",
    "                            y_real=test_y_real)\n",
    "    reportBootstrapTstat(yhats=test_yhats, y_real=test_y_real,\n",
    "        possible_positions=[-5, -4, -3, -2, -1, -0.75, 0.5, 0.25, 0, 0.25, 0.5, 0.75, 1, 2, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "3ce740c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " threshold is -0.005\n",
      "geom mean: 0.000004\n",
      "sharpe: 0.41\n",
      "count of num of trading windows where stop loss executed: 139\n",
      "count of num of trading windows where stop loss helped: 58\n",
      "arthimetic avg of how much stop loss helped conditional on helping: 0.005506\n",
      "\n",
      " threshold is -0.01\n",
      "geom mean: 0.000117\n",
      "sharpe: 1.00\n",
      "count of num of trading windows where stop loss executed: 63\n",
      "count of num of trading windows where stop loss helped: 21\n",
      "arthimetic avg of how much stop loss helped conditional on helping: 0.006237\n",
      "\n",
      " threshold is -0.02\n",
      "geom mean: 0.000211\n",
      "sharpe: 1.42\n",
      "count of num of trading windows where stop loss executed: 16\n",
      "count of num of trading windows where stop loss helped: 6\n",
      "arthimetic avg of how much stop loss helped conditional on helping: 0.005621\n",
      "\n",
      " threshold is -0.03\n",
      "geom mean: 0.000254\n",
      "sharpe: 1.59\n",
      "count of num of trading windows where stop loss executed: 4\n",
      "count of num of trading windows where stop loss helped: 0\n",
      "\n",
      " threshold is -0.04\n",
      "geom mean: 0.000316\n",
      "sharpe: 1.87\n",
      "count of num of trading windows where stop loss executed: 1\n",
      "count of num of trading windows where stop loss helped: 0\n",
      "\n",
      " threshold is -0.05\n",
      "geom mean: 0.000316\n",
      "sharpe: 1.87\n",
      "count of num of trading windows where stop loss executed: 1\n",
      "count of num of trading windows where stop loss helped: 0\n",
      "\n",
      " threshold is -0.06\n",
      "geom mean: 0.000396\n",
      "sharpe: 2.25\n",
      "count of num of trading windows where stop loss executed: 0\n",
      "count of num of trading windows where stop loss helped: 0\n",
      "\n",
      " threshold is -0.07\n",
      "geom mean: 0.000396\n",
      "sharpe: 2.25\n",
      "count of num of trading windows where stop loss executed: 0\n",
      "count of num of trading windows where stop loss helped: 0\n",
      "\n",
      " threshold is -0.08\n",
      "geom mean: 0.000396\n",
      "sharpe: 2.25\n",
      "count of num of trading windows where stop loss executed: 0\n",
      "count of num of trading windows where stop loss helped: 0\n",
      "\n",
      " threshold is -0.09\n",
      "geom mean: 0.000396\n",
      "sharpe: 2.25\n",
      "count of num of trading windows where stop loss executed: 0\n",
      "count of num of trading windows where stop loss helped: 0\n",
      "\n",
      " threshold is -0.1\n",
      "geom mean: 0.000396\n",
      "sharpe: 2.25\n",
      "count of num of trading windows where stop loss executed: 0\n",
      "count of num of trading windows where stop loss helped: 0\n"
     ]
    }
   ],
   "source": [
    "# DRAFT CODE BELOW FOR STOP LOSS\n",
    "\n",
    "# obtain validation period returns and positions\n",
    "y_df = df[val_start_date:val_end_date][[lhs_col]]\n",
    "y_df['yhats'] = opt_model_dict['yhats']*opt_model_dict['lev']\n",
    "\n",
    "# pull in minute level data\n",
    "min_df = pd.read_pickle('../1-data/clean/panel_ethbtc_1min.pkl')\n",
    "\n",
    "# form minute level return \n",
    "min_df['y_1min'] = min_df.price.pct_change()\n",
    "\n",
    "# cut down min df to window that i have\n",
    "min_df = min_df[min_df.date >= np.min(y_df.index)]\n",
    "min_df = min_df.set_index('date')\n",
    "min_df = min_df[:'2022-06-30']\n",
    "min_df = min_df.reset_index()\n",
    "\n",
    "# drop cols and reset index\n",
    "min_df = min_df[['date', 'y_1min']].reset_index(drop=True)\n",
    "y_df = y_df.reset_index()\n",
    "\n",
    "# merge data together\n",
    "y_df = y_df.merge(min_df,\n",
    "    on='date',\n",
    "    how='outer',\n",
    "    validate='one_to_one')\n",
    "y_df = y_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "# calc portfolio returns for no stop loss\n",
    "val_df = y_df[((y_df.date.dt.hour % 14) == 0) & ((y_df.date.dt.minute % 60) == 0)]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "yhats  = val_df.yhats.values[:-1]\n",
    "y_real = val_df[lhs_col].values[:-1]\n",
    "tc = calcTransactionCosts(yhats)\n",
    "returns = yhats*y_real - tc\n",
    "geom_mean = calcGeomAvg(returns)\n",
    "sharpe    = calcSharpe(returns, periods_in_year=int(365/2))\n",
    "print(f\"geom mean: {geom_mean:.6f}\")\n",
    "print(f\"sharpe: {sharpe:.2f}\")\n",
    "\n",
    "# loop over possible thresholds\n",
    "for threshold in [-0.005, -0.01, -0.02, -0.03, -0.04, -0.05, -0.06, -0.07, -0.08, -0.09, -0.1]:\n",
    "    print(f\"\\n threshold is {threshold}\")\n",
    "\n",
    "    # initialize array of returns to calc\n",
    "    y_real = []\n",
    "    count = 0\n",
    "    count_helpful = 0\n",
    "    total_help      = 0\n",
    "\n",
    "    # loop over every trading window\n",
    "    for start_index in range(0, y_df.shape[0]-120, 120):\n",
    "        # calc end index for this trading window\n",
    "        end_index = start_index+130 \n",
    "\n",
    "        # extract position\n",
    "        position = y_df[start_index:start_index+1].yhats.values[0]\n",
    "\n",
    "        # calc the cum ret over the window from five min post 2 hours to 130 min post 2 hours\n",
    "        cum_ret = np.cumprod(1+y_df[start_index+6:end_index+1].y_1min.values)-1\n",
    "\n",
    "        # if position is to be negative then flip returns\n",
    "        if position < 0:\n",
    "            cum_ret *= -1\n",
    "        \n",
    "        # if position is to not trade then set return to 0\n",
    "        if position == 0:\n",
    "            y_real.append(0)\n",
    "        else: # figure out stop loss if it comes in play\n",
    "            stop_loss = np.where(cum_ret<threshold)[0]\n",
    "            if len(stop_loss) == 0:\n",
    "                y_real.append(cum_ret[-1])\n",
    "            else:\n",
    "                index = np.min(stop_loss)\n",
    "                if index == 124:\n",
    "                    y_real.append(cum_ret[index])\n",
    "                else:\n",
    "                    y_real.append(cum_ret[index+1]) \n",
    "                    count += 1\n",
    "                    if cum_ret[index+1] > cum_ret[124]:\n",
    "                        count_helpful += 1\n",
    "                        total_help += cum_ret[index+1] - cum_ret[124]\n",
    "\n",
    "    returns = np.abs(yhats)*y_real - tc\n",
    "    geom_mean = calcGeomAvg(returns)\n",
    "    sharpe    = calcSharpe(returns, periods_in_year=365*12)\n",
    "    print(f\"geom mean: {geom_mean:.6f}\")\n",
    "    print(f\"sharpe: {sharpe:.2f}\") \n",
    "    print(f\"count of num of trading windows where stop loss executed: {count}\")\n",
    "    print(f\"count of num of trading windows where stop loss helped: {count_helpful}\")\n",
    "    if count_helpful > 0:\n",
    "        print(f\"arthimetic avg of how much stop loss helped conditional on helping: {total_help/count_helpful:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
